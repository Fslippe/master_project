{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Flatten, Reshape\n",
    "from sklearn.feature_extraction import image as sk_image\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from keras.models import Model\n",
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.signal import convolve2d \n",
    "from scipy import ndimage\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras    \n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pyhdf.SD import SD, SDC\n",
    "import matplotlib as mpl\n",
    "#tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "from extract_training_data import *\n",
    "from sklearn.feature_extraction.image import extract_patches_2d, reconstruct_from_patches_2d\n",
    "from pyhdf.error import HDF4Error\n",
    "from functions import *\n",
    "from tensorflow.keras.models import load_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('training_history.pkl', 'rb') as f:\n",
    "    loaded_history = pickle.load(f)\n",
    "\n",
    "plt.plot(loaded_history[\"loss\"])\n",
    "plt.plot(loaded_history[\"val_loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "\n",
    "#bands = [6, 7, 20, 28, 28, 31]\n",
    "bands=[29]\n",
    "#bands=[1]\n",
    "folder = \"/scratch/fslippe/modis/MOD02/daytime_1km/ /scratch/fslippe/modis/MOD02/boundary_1km/ /scratch/fslippe/modis/MOD02/night_1km/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import extract_training_data\n",
    "importlib.reload(extract_training_data)\n",
    "from extract_training_data import *\n",
    "#folder = \"/scratch/fslippe/modis/MOD02/daytime_1km/\"# /scratch/fslippe/modis/MOD02/boundary_1km/\"# /scratch/fslippe/modis/MOD02/night_1km/\"\n",
    "\n",
    "start = \"20201201\"\n",
    "end = \"20210430\"\n",
    "# start = \"20210401\"\n",
    "# end = \"20210430\"\n",
    "\n",
    "\n",
    "start_converted = convert_to_day_of_year(start)\n",
    "end_converted = convert_to_day_of_year(end)\n",
    "print(start_converted)\n",
    "print(end_converted)\n",
    "x, dates, masks = extract_1km_data(folder, bands=bands, start_date=start_converted, end_date=end_converted)\n",
    "x, dates, masks = zip(*[(xi, date, mask) for xi, date, mask in zip(x, dates, masks) if (xi.shape[0] > 64) and (xi.shape[1] > 64)])\n",
    "x = list(x)\n",
    "dates = list(dates)\n",
    "\n",
    "#x = extract_250m_data(folder, bands=[1], start_date=start_converted, end_date=end_converted)\n",
    "len(masks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoencoder\n",
    "import importlib\n",
    "importlib.reload(autoencoder)\n",
    "from autoencoder import SobelFilterLayer, SimpleAutoencoder\n",
    "patch_size = 64\n",
    "#normalized_patches = np.concatenate([autoencoder.extract_patches(n_d) for n_d in normalized_data], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import SobelFilterLayer, SimpleAutoencoder\n",
    "patch_size = 64\n",
    "print(len(bands))\n",
    "autoencoder_predict = SimpleAutoencoder(len(bands), patch_size, patch_size)\n",
    "\n",
    "#encoder = load_model(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(6,20,29)_encoder\")\n",
    "encoder = load_model(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_dnb_band(29)_filter_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_vals = np.load(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(6,20,29)_max_vals.npy\")\n",
    "max_vals = np.array([15.703261])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### EXTRACTING AND ENCODING PATCHES + SAVING indexes of belonging files\n",
    "cluster_map = []\n",
    "all_patches = []\n",
    "starts = []\n",
    "ends =[]\n",
    "shapes = []\n",
    "start = 0 \n",
    "\n",
    "for (image, mask) in zip(x[:100], masks[:100]):\n",
    "    shapes.append(image.shape[0:2])\n",
    "    patches, idx, n_patches = autoencoder_predict.extract_patches(image, mask, mask_threshold=0.9)  # Assuming this function extracts and reshapes patches for a single image\n",
    "    all_patches.append(patches)\n",
    "    starts.append(start)\n",
    "    ends.append(start + n_patches)\n",
    "    start += len(patches)\n",
    "\n",
    "# Stack filtered patches from all images\n",
    "patches = np.concatenate(all_patches, axis=0) / max_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_patches = encoder.predict(patches)\n",
    "encoded_patches_flat = encoded_patches.reshape(encoded_patches.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXTRACT CAO AND NOn CAO CASES\n",
    "import importlib\n",
    "import extract_training_data\n",
    "importlib.reload(extract_training_data)\n",
    "from extract_training_data import *\n",
    "start = \"20230303\"\n",
    "end = \"20230306\"\n",
    "\n",
    "start = \"20210701\"\n",
    "end = \"20210702\"\n",
    "start_converted = convert_to_day_of_year(start)\n",
    "end_converted = convert_to_day_of_year(end)\n",
    "x_cao, dates_cao, masks_cao = extract_1km_data(\"/scratch/fslippe/modis/MOD02/july_2021/\", bands=bands, start_date=start_converted, end_date=end_converted)\n",
    "\n",
    "#x_cao, dates_cao, masks_cao = extract_1km_data(\"/scratch/fslippe/modis/MOD02/cao_test_data/\", bands=bands, start_date=start_converted, end_date=end_converted)\n",
    "x_cao, dates_cao, masks_cao = zip(*[(xi, date, mask) for xi, date, mask in zip(x_cao, dates_cao, masks_cao) if (xi.shape[0] > 64) and (xi.shape[1] > 64)])\n",
    "# x_cao, dates_cao = extract_1km_data(\"/scratch/fslippe/modis/MOD02/cao_test_data/\", bands=bands, start_date=start_converted, end_date=end_converted)\n",
    "# x_cao, dates_cao = zip(*[(xi, date) for xi, date in zip(x_cao, dates_cao) if (xi.shape[0] > 64) and (xi.shape[1] > 64)])\n",
    "x_cao = list(x_cao)\n",
    "dates_cao = list(dates_cao)\n",
    "\n",
    "# start = \"20210701\"\n",
    "# end = \"20210702\"\n",
    "\n",
    "# start = \"20210401\"\n",
    "# end = \"20210402\"\n",
    "# start_converted = convert_to_day_of_year(start)\n",
    "# end_converted = convert_to_day_of_year(end)\n",
    "# start_converted = convert_to_day_of_year(start)\n",
    "# end_converted = convert_to_day_of_year(end)\n",
    "# x_no, dates_no, masks_no = extract_1km_data(folder, bands=bands, start_date=start_converted, end_date=end_converted)\n",
    "# x_no, dates_no, masks_no = zip(*[(xi, date, mask) for xi, date, mask in zip(x_no, dates_no, masks_no) if (xi.shape[0] > 64) and (xi.shape[1] > 64)])\n",
    "# x_no = list(x_no)\n",
    "# dates_no = list(dates_no)\n",
    "# #x2 = [xi for xi in  extract_1km_data(folder, bands=[1], start_date=start_converted, end_date=end_converted) if xi.shape[0] > 64]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### EXTRACTING AND ENCODING PATCHES + SAVING indexes of belonging files\n",
    "cluster_map_cao = []\n",
    "all_patches_cao = []\n",
    "starts_cao = []\n",
    "ends_cao =[]\n",
    "shapes_cao = []\n",
    "start_cao = 0 \n",
    "n_patches_tot = []\n",
    "indices = []\n",
    "\n",
    "for (image, mask) in zip(x_cao, masks_cao):\n",
    "    shapes_cao.append(image.shape[0:2])\n",
    "    patches_cao, idx_cao, n_patches = autoencoder_predict.extract_patches(image, mask, mask_threshold=0.9)  # Assuming this function extracts and reshapes patches for a single image\n",
    "    #patches_cao = autoencoder_predict.extract_patches(image)  # Assuming this function extracts and reshapes patches for a single image\n",
    "    \n",
    "    n_patches_cao = len(patches_cao)\n",
    "    all_patches_cao.append(patches_cao)\n",
    "    starts_cao.append(start_cao)\n",
    "    ends_cao.append(start_cao + n_patches_cao)\n",
    "    n_patches_tot.append(n_patches)\n",
    "    indices.append(idx_cao)\n",
    "    start_cao += n_patches_cao\n",
    "\n",
    "# Stack filtered patches from all images\n",
    "patches_cao = np.concatenate(all_patches_cao, axis=0) / max_vals\n",
    "\n",
    "encoded_patches_cao = encoder.predict(patches_cao)\n",
    "encoded_patches_flat_cao = encoded_patches_cao.reshape(encoded_patches_cao.shape[0], -1)\n",
    "\n",
    "\n",
    "# cluster_map_no = []\n",
    "# all_patches_no = []\n",
    "# starts_no = []\n",
    "# ends_no =[]\n",
    "# shapes_no = []\n",
    "# start_no = 0 \n",
    "\n",
    "# for image in x_no:\n",
    "#     shapes_no.append(image.shape[0:2])\n",
    "#     patches_no = autoencoder_predict.extract_patches(image)  # Assuming this function extracts and reshapes patches for a single image\n",
    "#     all_patches_no.append(patches_no)\n",
    "#     starts_no.append(start_no)\n",
    "#     ends_no.append(start_no + len(patches_no))\n",
    "#     start_no += len(patches_no)\n",
    "#     # Calculate the dimensions of the reduced resolution array\n",
    "# # Stack filtered patches from all images\n",
    "# patches_no = np.concatenate(all_patches_no, axis=0) / max_vals\n",
    "# encoded_patches_no = encoder.predict(patches_no)\n",
    "# encoded_patches_flat_no = encoded_patches_no.reshape(encoded_patches_no.shape[0], -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import joblib\n",
    "\n",
    "#cluster = MiniBatchKMeans(11, batch_size=32, random_state=42).fit(encoded_patches_flat)\n",
    "#joblib.dump(cluster, '/uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(29)_filter_cluster_dnb_lab0.pkl')\n",
    "cluster = joblib.load('/uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(29)_filter_cluster_dnb_lab0.pkl')\n",
    "\n",
    "#decoder = load_model(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_dnb_band(29)_filter_encoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "index_list = [0,1,2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15,16,17,18,19]\n",
    "index_list = range(len(x_cao))\n",
    "#index_list = [2]\n",
    "\n",
    "cluster_predict = cluster.predict(encoded_patches_flat_cao)\n",
    "labels = cluster_predict#.labels_\n",
    "cluster_map_cao = []\n",
    "\n",
    "global_min = np.min([np.min(cm) for cm in cluster.labels_])\n",
    "global_max = np.max([np.max(cm) for cm in cluster.labels_])+2\n",
    "norm = Normalize(vmin=global_min, vmax=global_max)  \n",
    "print(global_max)\n",
    "\n",
    "norm_mask = Normalize(vmin=0, vmax=1)  \n",
    "\n",
    "cmap_tab10 = plt.cm.tab10\n",
    "colors_tab10 = cmap_tab10(np.arange(cmap_tab10.N))\n",
    "\n",
    "# Add black to the end\n",
    "black = np.array([0, 0, 0, 1])\n",
    "colors_new = np.vstack((colors_tab10, black))\n",
    "\n",
    "# Create a new colormap from the combined list of colors\n",
    "new_cmap = mcolors.ListedColormap(colors_new)\n",
    "# boundaries = np.arange(global_max+1) \n",
    "# norm = mcolors.BoundaryNorm(boundaries, new_cmap.N, clip=True)\n",
    "# Assuming your original data shape is (height, width)\n",
    "for i in index_list:\n",
    "    height, width = shapes_cao[i]\n",
    "\n",
    "    # Calculate the dimensions of the reduced resolution array\n",
    "    reduced_height = height // patch_size\n",
    "    reduced_width = width //patch_size\n",
    "    current_labels = np.ones((n_patches_tot[i]))*(global_max+1)\n",
    "    print(labels[starts_cao[i]:ends_cao[i]].shape)\n",
    "\n",
    "    current_labels[np.squeeze(indices[i].numpy())] = labels[starts_cao[i]:ends_cao[i]]\n",
    "   \n",
    "    cluster_map_cao.append(np.reshape(current_labels, (reduced_height, reduced_width)))\n",
    "\n",
    "    fig, axs = plt.subplots(1,3, figsize=(25, 6))\n",
    "    fig.suptitle(\"idx:%s, %s, %s, %s\" %(i, np.max(current_labels), np.min(current_labels), np.mean(current_labels)))\n",
    "    axs[0].imshow(x_cao[i], cmap=\"gray\")\n",
    "    cb = axs[1].imshow(cluster_map_cao[i], cmap=new_cmap, norm=norm)\n",
    "    plt.colorbar(cb)\n",
    "    axs[2].imshow(masks_cao[i], norm=norm_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_label = 0\n",
    "size_threshold = 7  # Adjust based on the minimum size of the region you are interested in\n",
    "selected_dates = []\n",
    "selected_images = []\n",
    "\n",
    "for image, date, label_map in zip(x_cao, dates_cao, cluster_map_cao):\n",
    "    # Identify regions where the desired label is present\n",
    "    binary_map = (label_map == desired_label)\n",
    "\n",
    "    # Label connected components\n",
    "    labeled_map, num_features = ndimage.label(binary_map)\n",
    "\n",
    "    # Measure sizes of connected components\n",
    "    region_sizes = ndimage.sum(binary_map, labeled_map, range(num_features + 1))\n",
    "\n",
    "    # Check if any region exceeds the size threshold\n",
    "    if any(region_sizes > size_threshold):\n",
    "        selected_images.append(image)\n",
    "        if date not in selected_dates:\n",
    "            selected_dates.append(date) \n",
    "        fig, axs = plt.subplots(1,2)\n",
    "        fig.suptitle(\"ENOUGH GREEN\")\n",
    "        axs[0].imshow(image, cmap=\"gray\")\n",
    "        axs[1].imshow(label_map, cmap=\"tab10\")\n",
    "    else:\n",
    "        fig, axs = plt.subplots(1,2)\n",
    "        fig.suptitle(\"NOT ENOUGH\")\n",
    "        axs[0].imshow(image, cmap=\"gray\")\n",
    "        axs[1].imshow(label_map, cmap=\"tab10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [0,1,2, 3, 4, 5, 6, 7, 8, 9,10]\n",
    "cluster_predict = cluster.predict(encoded_patches_flat_no)\n",
    "labels = cluster_predict#.labels_\n",
    "\n",
    "# Assuming your original data shape is (height, width)\n",
    "for i in range(len(x_no)):\n",
    "    height, width = shapes_no[i]\n",
    "\n",
    "    # Calculate the dimensions of the reduced resolution array\n",
    "    reduced_height = height // patch_size\n",
    "    reduced_width = width //patch_size\n",
    "    cluster_map_no.append(np.reshape(labels[starts_no[i]:ends_no[i]], (reduced_height, reduced_width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_cao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global_min = np.min([np.min(cm) for cm in cluster.labels_])\n",
    "global_max = np.max([np.max(cm) for cm in cluster.labels_])\n",
    "du -\n",
    "norm = Normalize(vmin=global_min, vmax=global_max)  \n",
    "for i in range(len(x_no)):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=[10, 5])\n",
    "    fig.suptitle(i)\n",
    "    cb = axs[0].imshow(cluster_map_no[i], cmap=\"tab10\", norm=norm)\n",
    "    plt.colorbar(cb, ax=axs[0])\n",
    "    cb = axs[1].imshow(x_no[i][::2, ::2, 0], cmap=\"gray\")\n",
    "    axs[1].set_title(\"band 29\")\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global_min = np.min([np.min(cm) for cm in cluster.labels_])\n",
    "global_max = np.max([np.max(cm) for cm in cluster.labels_])\n",
    "\n",
    "norm = Normalize(vmin=global_min, vmax=global_max)  \n",
    "for i in range(len(x_cao)):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=[10, 5], dpi=250)\n",
    "    fig.suptitle(i)\n",
    "    cb = axs[0].imshow(cluster_map_cao[i], cmap=\"tab10\", norm=norm)\n",
    "    plt.colorbar(cb, ax=axs[0])\n",
    "    cb = axs[1].imshow(x_cao[i][:, :, 0], cmap=\"gray\")\n",
    "    axs[1].set_title(\"band 29\")\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
