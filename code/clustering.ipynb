{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "from keras.layers import Input, Dense, Flatten, Reshape\n",
    "from sklearn.feature_extraction import image as sk_image\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import cartopy.feature as cfeature\n",
    "from keras.models import Model\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.signal import convolve2d \n",
    "from scipy import ndimage\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras    \n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pyhdf.SD import SD, SDC\n",
    "import matplotlib as mpl\n",
    "#tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "from extract_training_data import *\n",
    "from sklearn.feature_extraction.image import extract_patches_2d, reconstruct_from_patches_2d\n",
    "from pyhdf.error import HDF4Error\n",
    "from functions import *\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import joblib\n",
    "import plot_functions\n",
    "import importlib \n",
    "importlib.reload(plot_functions)\n",
    "from plot_functions import *\n",
    "# Visualize the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('training_history_landmask_150k_1e3.pkl', 'rb') as f:\n",
    "    loaded_history = pickle.load(f)\n",
    "\n",
    "with open('training_history_landmask_1e-4_150k.pkl', 'rb') as f:\n",
    "    loaded_history_2 = pickle.load(f)\n",
    "with open('training_history_landmask_150k.pkl', 'rb') as f:\n",
    "    loaded_history_3 = pickle.load(f)\n",
    "\n",
    "plt.plot(loaded_history[\"loss\"][30:150], \"b\", label=\"1e-3 loss\")\n",
    "plt.plot(loaded_history[\"val_loss\"][30:150], \"b--\", label=\"1e-3 val loss\")\n",
    "\n",
    "plt.plot(loaded_history_2[\"loss\"][30:150], \"r\", label=\"1e-4 loss\")\n",
    "plt.plot(loaded_history_2[\"val_loss\"][30:150], \"r--\", label=\"1e-4 val loss\")\n",
    "\n",
    "plt.plot(loaded_history_3[\"loss\"][30:150], \"k\", label=\"scheduler loss\")\n",
    "plt.plot(loaded_history_3[\"val_loss\"][30:150], \"k--\", label=\"scheduler val loss\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\"  , len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "\n",
    "#bands = [6, 7, 20, 28, 28, 31]\n",
    "bands=[29]\n",
    "#bands=[1]\n",
    "folder = \"/scratch/fslippe/modis/MOD02/daytime_1km/ /scratch/fslippe/modis/MOD02/boundary_1km/ /scratch/fslippe/modis/MOD02/night_1km/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import extract_training_data\n",
    "importlib.reload(extract_training_data)\n",
    "from extract_training_data import *\n",
    "#folder = \"/scratch/fslippe/modis/MOD02/daytime_1km/\"# /scratch/fslippe/modis/MOD02/boundary_1km/\"# /scratch/fslippe/modis/MOD02/night_1km/\"\n",
    "\n",
    "start = \"20201201\"\n",
    "end = \"20210430\"\n",
    "# start = \"20210401\"\n",
    "# end = \"20210430\"\n",
    "\n",
    "\n",
    "start_converted = convert_to_day_of_year(start)\n",
    "end_converted = convert_to_day_of_year(end)\n",
    "print(start_converted)\n",
    "print(end_converted)\n",
    "x, dates, masks = extract_1km_data(folder, bands=bands, start_date=start_converted, end_date=end_converted)\n",
    "x, dates, masks = zip(*[(xi, date, mask) for xi, date, mask in zip(x, dates, masks) if (xi.shape[0] > 64) and (xi.shape[1] > 64)])\n",
    "x = list(x)\n",
    "dates = list(dates)\n",
    "\n",
    "#x = extract_250m_data(folder, bands=[1], start_date=start_converted, end_date=end_converted)\n",
    "len(masks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoencoder\n",
    "import importlib\n",
    "importlib.reload(autoencoder)\n",
    "from autoencoder import SobelFilterLayer, SimpleAutoencoder\n",
    "patch_size = 64\n",
    "#normalized_patches = np.concatenate([autoencoder.extract_patches(n_d) for n_d in normalized_data], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import SobelFilterLayer, SimpleAutoencoder\n",
    "patch_size = 64\n",
    "print(len(bands))\n",
    "autoencoder_predict = SimpleAutoencoder(len(bands), patch_size, patch_size)\n",
    "\n",
    "#encoder = load_model(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(6,20,29)_encoder\")\n",
    "encoder = load_model(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_dnb_landmask_150k_scheduler_band(29)_filter_encoder\")\n",
    "max_vals = np.load(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_dnb_landmask_band(29)_max_vals.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = np.load(\"/scratch/fslippe/modis/MOD02/test_data/normalized_testpatches_dnb_landmask_150k_band(29)_winter20_21.npy\")\n",
    "encoded_patches = encoder.predict(val_data)\n",
    "encoded_patches_flat = encoded_patches.reshape(encoded_patches.shape[0], -1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster = MiniBatchKMeans(11, batch_size=32, random_state=42).fit(encoded_patches_flat)\n",
    "#joblib.dump(cluster, '/uio/hume/student-u37/fslippe/data/models/winter_2020_21_dnb_landmask_150k_scheduler_band(29)_filter_cluster.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXTRACT CAO AND NOn CAO CASES\n",
    "import importlib\n",
    "import extract_training_data\n",
    "importlib.reload(extract_training_data)\n",
    "from extract_training_data import *\n",
    "# start = \"20230303\"\n",
    "# end = \"20230306\"\n",
    "start = \"20210101\"\n",
    "end = \"20210401\"\n",
    "#start = \"20210701\"\n",
    "#end = \"20210702\"\n",
    "start_converted = convert_to_day_of_year(start)\n",
    "end_converted = convert_to_day_of_year(end)\n",
    "#x_cao, dates_cao, masks_cao = extract_1km_data(\"/scratch/fslippe/modis/MOD02/july_2021/\", bands=bands, start_date=start_converted, end_date=end_converted)\n",
    "\n",
    "# x_cao, dates_cao, masks_cao, lon_lats = extract_1km_data(\"/scratch/fslippe/modis/MOD02/cao_test_data/\",\n",
    "#                                                          bands=bands,\n",
    "#                                                          start_date=start_converted,\n",
    "#                                                          end_date=end_converted,\n",
    "#                                                          return_lon_lat=True)\n",
    "x_cao, dates_cao, masks_cao, lon_lats = extract_1km_data(folder,\n",
    "                                                         bands=bands,\n",
    "                                                         start_date=start_converted,\n",
    "                                                         end_date=end_converted,\n",
    "                                                         return_lon_lat=True)\n",
    "x_cao, dates_cao, masks_cao, lon_lats = zip(*[(xi, date, mask, lon_lat) for xi, date, mask, lon_lat in zip(x_cao, dates_cao, masks_cao, lon_lats) if (xi.shape[0] > 64) and (xi.shape[1] > 64)])\n",
    "\n",
    "x_cao = list(x_cao)\n",
    "dates_cao = list(dates_cao)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### EXTRACTING AND ENCODING PATCHES + SAVING indexes of belonging files\n",
    "import functions\n",
    "importlib.reload(functions)\n",
    "from functions import *\n",
    "cluster_map_cao = []\n",
    "patches_cao, all_lon_patches_cao, all_lat_patches_cao, starts_cao, ends_cao, shapes_cao, n_patches_tot_cao, indices = generate_patches(x_cao,\n",
    "                                                                                                                                       masks_cao,\n",
    "                                                                                                                                       lon_lats,\n",
    "                                                                                                                                       max_vals,\n",
    "                                                                                                                                       autoencoder_predict)\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [0,1,2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15,16,17,18,19]\n",
    "#index_list = range(len(x_cao))\n",
    "#index_list = [2]\n",
    "\n",
    "cluster_predict = cluster.predict(encoded_patches_flat_cao)\n",
    "labels = cluster_predict#.labels_\n",
    "cluster_map_cao = []\n",
    "\n",
    "global_min = np.min([np.min(cm) for cm in cluster.labels_])\n",
    "global_max = np.max([np.max(cm) for cm in cluster.labels_])+2\n",
    "print(global_max)\n",
    "plot_img_cluster_mask(x_cao,\n",
    "                      labels,\n",
    "                      masks_cao,\n",
    "                      starts_cao,\n",
    "                      ends_cao,\n",
    "                      shapes_cao,\n",
    "                      indices,\n",
    "                      dates_cao,\n",
    "                      n_patches_tot_cao,\n",
    "                      patch_size,\n",
    "                      global_min,\n",
    "                      global_max,\n",
    "                      index_list,\n",
    "                      save=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "march_idx = np.where(np.array(dates_cao) == str(convert_to_day_of_year(\"20210301\")))[0][0]\n",
    "projection = ccrs.Stereographic(central_latitude=90)\n",
    "\n",
    "x_grid, y_grid, counts = generate_hist_map(n_patches_tot_cao,\n",
    "                                        indices,\n",
    "                                        labels,\n",
    "                                        starts_cao,\n",
    "                                        ends_cao,  \n",
    "                                        shapes_cao,\n",
    "                                        all_lon_patches_cao,\n",
    "                                        all_lat_patches_cao,  \n",
    "                                        dates_cao,\n",
    "                                        desired_label=2,\n",
    "                                        size_threshold=15,\n",
    "                                        patch_size=patch_size,\n",
    "                                        global_max=global_max,\n",
    "                                        projection=projection,\n",
    "                                        grid_resolution = 100e3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result\n",
    "\n",
    "tot_days = int(dates_cao[-1]) - int(dates_cao[0])\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': projection}, figsize=(15, 8), dpi=200)\n",
    "plt.title(\"Percentage of time with predicted CAO in Jan/Feb\")\n",
    "ax.set_extent([-35, 35, 55, 84], ccrs.PlateCarree())  # Set extent to focus on the Arctic\n",
    "ax.add_feature(cfeature.LAND, edgecolor='black')\n",
    "ax.add_feature(cfeature.OCEAN)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "c = ax.contourf(x_grid, y_grid, counts/tot_days*100, transform=projection, levels=10, cmap=\"turbo\")\n",
    "plt.colorbar(c, ax=ax, orientation='vertical', label='[%]')\n",
    "ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
