{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cores: 256\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.layers import Input, Dense, Flatten, Reshape\n",
    "from sklearn.feature_extraction import image as sk_image\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from keras.models import Model\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.signal import convolve2d\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pyhdf.SD import SD, SDC\n",
    "import matplotlib as mpl\n",
    "#tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "from extract_training_data import *\n",
    "from sklearn.feature_extraction.image import extract_patches_2d, reconstruct_from_patches_2d\n",
    "from pyhdf.error import HDF4Error\n",
    "from functions import *\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 16:48:33.716135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21336 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:25:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)\n",
    "\n",
    "#bands = [6, 7, 20, 28, 28, 31]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bands = [6,20,29]\n",
    "#bands = [6,7,20,28,29,31]\n",
    "bands = [29]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cores: 256\n",
      "/scratch/fslippe/modis/MOD02/daytime_1km/MOD021KM.A2020336.0800.061.2020337214455.hdf\n",
      "0 Latitude\n",
      "1 Longitude\n",
      "2 EV_1KM_RefSB\n",
      "3 EV_1KM_RefSB_Uncert_Indexes\n",
      "4 EV_1KM_Emissive\n",
      "5 EV_1KM_Emissive_Uncert_Indexes\n",
      "6 EV_250_Aggr1km_RefSB\n",
      "7 EV_250_Aggr1km_RefSB_Uncert_Indexes\n",
      "8 EV_250_Aggr1km_RefSB_Samples_Used\n",
      "9 EV_500_Aggr1km_RefSB\n",
      "10 EV_500_Aggr1km_RefSB_Uncert_Indexes\n",
      "11 EV_500_Aggr1km_RefSB_Samples_Used\n",
      "12 Height\n",
      "13 SensorZenith\n",
      "14 SensorAzimuth\n",
      "15 Range\n",
      "16 SolarZenith\n",
      "17 SolarAzimuth\n",
      "18 gflags\n",
      "19 EV_Band26\n",
      "20 EV_Band26_Uncert_Indexes\n",
      "21 Band_250M\n",
      "22 Band_500M\n",
      "23 Band_1KM_RefSB\n",
      "24 Band_1KM_Emissive\n",
      "25 Noise in Thermal Detectors\n",
      "26 Change in relative responses of thermal detectors\n",
      "27 DC Restore Change for Thermal Bands\n",
      "28 DC Restore Change for Reflective 250m Bands\n",
      "29 DC Restore Change for Reflective 500m Bands\n",
      "30 DC Restore Change for Reflective 1km Bands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124/124 [00:21<00:00,  5.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "622"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PROCESSING .hdf files\n",
    "import extract_training_data\n",
    "import importlib\n",
    "importlib.reload(extract_training_data)\n",
    "from extract_training_data import *\n",
    "#folder = \"/uio/hume/student-u37/fslippe/data/training_data/MOD02QKM/normalized_data/\"\n",
    "folder = \"/scratch/fslippe/modis/MOD02/daytime_1km/\"\n",
    "\n",
    "#folder = \"/uio/hume/student-u37/fslippe/data/nird_mount/MOD02QKM_202012-202104/normalized_data/\"\n",
    "\n",
    "start = \"20201201\"\n",
    "end = \"20210430\"\n",
    "#end = \"20201230\"\n",
    "\n",
    "#dates = [\"20210303\"]#, \"20210322\"]#, \"20210323\"]\n",
    "#dates_converted = []\n",
    "#for date in dates:\n",
    "#    dates_converted.append(convert_to_day_of_year(date))\n",
    "#bands = [6,20,29]\n",
    "\n",
    "start_converted = convert_to_day_of_year(start)\n",
    "end_converted = convert_to_day_of_year(end)\n",
    "#print(start_converted)\n",
    "#print(end_converted)\n",
    "#x = [xi for xi in  extract_250m_data(folder, bands=[1], date_list=dates_converted) if xi.shape[0] > 256]\n",
    "x = [xi for xi in  extract_1km_data(folder, bands=bands, start_date=start_converted, end_date=end_converted) if xi.shape[0] > 64]\n",
    "\n",
    "len(x)\n",
    "#x = extract_250m_data(folder, bands=[1], start_date=start_converted, end_date=end_converted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input should already be normalized. Call self.normalize to normalize list of data\n"
     ]
    }
   ],
   "source": [
    "import autoencoder\n",
    "import importlib\n",
    "importlib.reload(autoencoder)\n",
    "from autoencoder import SobelFilterLayer, SimpleAutoencoder\n",
    "patch_size = 64\n",
    "\n",
    "autoencoder = SimpleAutoencoder(len(bands), patch_size, patch_size)\n",
    "#x = autoencoder.normalize(x)\n",
    "#optimizer = mixed_precision.LossScaleOptimizer(tf.keras.optimizers.Adam(learning_rate=1e-4), loss_scale='dynamic')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "model = autoencoder.model(optimizer=optimizer, loss=\"combined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147502, 64, 64, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = np.concatenate([autoencoder.extract_patches(n_d) for n_d in x], axis=0)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished split\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "patches, val_data = train_test_split(patches, test_size=0.1, random_state=42)\n",
    "print(\"finished split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132751, 64, 64, 1)\n",
      "(14751, 64, 64, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(patches.shape)\n",
    "print(val_data.shape)\n",
    "patches.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[15.703261]]]]\n"
     ]
    }
   ],
   "source": [
    "min_val = 0\n",
    "max_val = np.max(patches, axis=(0,1,2), keepdims=True)\n",
    "print(max_val)\n",
    "patches = (patches - min_val) / (max_val - min_val)\n",
    "val_data = (val_data - min_val) / (max_val - min_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"/scratch/fslippe/modis/MOD02/test_data/normalized_testpatches_band(1)_winter20_21.npy\", val_data)\n",
    "#np.save(\"/scratch/fslippe/modis/MOD02/training_data/normalized_trainingpatches_band(1)_winter20_21.npy\", patches)\n",
    "# val_data = np.load(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_val_patches.npy\")\n",
    "# patches = np.load(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_train_patches.npy\")[:int(170e3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "### SAVE TRAIN TEST PATCHES \n",
    "# patches = np.load(\"/scratch/fslippe/modis/MOD02/training_data/normalized_trainingpatches_bands6,20,29_winter20_21.npy\")[::4]\n",
    "# val_data  = np.load(\"/scratch/fslippe/modis/MOD02/test_data/normalized_testpatches_bands6,20,29_winter20_21.npy\")[::4]\n",
    "# patches.shape\n",
    "\n",
    "# print(np.mean(patches, axis=(0,1,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 16:50:25.023899: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4149/4149 [==============================] - 27s 6ms/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 2/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 3/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 0.0011 - val_loss: 9.3207e-04\n",
      "Epoch 4/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 8.8230e-04 - val_loss: 8.1193e-04\n",
      "Epoch 5/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 7.7076e-04 - val_loss: 7.0856e-04\n",
      "Epoch 6/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 6.9942e-04 - val_loss: 6.4968e-04\n",
      "Epoch 7/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 6.4690e-04 - val_loss: 6.0660e-04\n",
      "Epoch 8/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 6.0734e-04 - val_loss: 5.7040e-04\n",
      "Epoch 9/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 5.7432e-04 - val_loss: 5.5582e-04\n",
      "Epoch 10/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 5.4576e-04 - val_loss: 5.1127e-04\n",
      "Epoch 11/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 5.2294e-04 - val_loss: 5.1064e-04\n",
      "Epoch 12/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 5.0150e-04 - val_loss: 4.7282e-04\n",
      "Epoch 13/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 4.8348e-04 - val_loss: 4.5586e-04\n",
      "Epoch 14/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 4.6842e-04 - val_loss: 4.5136e-04\n",
      "Epoch 15/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 4.5350e-04 - val_loss: 5.1523e-04\n",
      "Epoch 16/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 4.4186e-04 - val_loss: 4.2430e-04\n",
      "Epoch 17/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 4.3185e-04 - val_loss: 4.3519e-04\n",
      "Epoch 18/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 4.2201e-04 - val_loss: 4.0907e-04\n",
      "Epoch 19/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 4.1359e-04 - val_loss: 3.9413e-04\n",
      "Epoch 20/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 4.0502e-04 - val_loss: 3.9401e-04\n",
      "Epoch 21/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 3.9693e-04 - val_loss: 4.1398e-04\n",
      "Epoch 22/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 3.8908e-04 - val_loss: 3.7153e-04\n",
      "Epoch 23/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 3.8246e-04 - val_loss: 3.7158e-04\n",
      "Epoch 24/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 3.7720e-04 - val_loss: 3.5921e-04\n",
      "Epoch 25/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 3.6963e-04 - val_loss: 3.5762e-04\n",
      "Epoch 26/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 3.6256e-04 - val_loss: 3.4906e-04\n",
      "Epoch 27/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 3.5667e-04 - val_loss: 3.5345e-04\n",
      "Epoch 28/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 3.5114e-04 - val_loss: 3.4377e-04\n",
      "Epoch 29/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 3.4614e-04 - val_loss: 3.7062e-04\n",
      "Epoch 30/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 3.4071e-04 - val_loss: 3.3923e-04\n",
      "Epoch 31/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 3.3621e-04 - val_loss: 3.1918e-04\n",
      "Epoch 32/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 3.3116e-04 - val_loss: 3.3337e-04\n",
      "Epoch 33/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 3.2687e-04 - val_loss: 3.1470e-04\n",
      "Epoch 34/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 3.2288e-04 - val_loss: 3.0983e-04\n",
      "Epoch 35/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 3.1966e-04 - val_loss: 3.1761e-04\n",
      "Epoch 36/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 3.1624e-04 - val_loss: 3.0156e-04\n",
      "Epoch 37/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 3.1369e-04 - val_loss: 3.0063e-04\n",
      "Epoch 38/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 3.1153e-04 - val_loss: 2.9509e-04\n",
      "Epoch 39/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 3.0801e-04 - val_loss: 2.9368e-04\n",
      "Epoch 40/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 3.0609e-04 - val_loss: 2.9276e-04\n",
      "Epoch 41/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 3.0291e-04 - val_loss: 2.9633e-04\n",
      "Epoch 42/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 3.0084e-04 - val_loss: 2.9466e-04\n",
      "Epoch 43/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.9848e-04 - val_loss: 2.8677e-04\n",
      "Epoch 44/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.9682e-04 - val_loss: 2.8305e-04\n",
      "Epoch 45/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.9576e-04 - val_loss: 2.7713e-04\n",
      "Epoch 46/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.9328e-04 - val_loss: 2.8351e-04\n",
      "Epoch 47/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.9168e-04 - val_loss: 2.8560e-04\n",
      "Epoch 48/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.9061e-04 - val_loss: 2.7791e-04\n",
      "Epoch 49/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.8928e-04 - val_loss: 3.0936e-04\n",
      "Epoch 50/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.8798e-04 - val_loss: 2.7869e-04\n",
      "Epoch 51/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.8642e-04 - val_loss: 2.7636e-04\n",
      "Epoch 52/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.8548e-04 - val_loss: 2.7373e-04\n",
      "Epoch 53/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.8398e-04 - val_loss: 2.7496e-04\n",
      "Epoch 54/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.8277e-04 - val_loss: 2.7412e-04\n",
      "Epoch 55/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.8267e-04 - val_loss: 2.7592e-04\n",
      "Epoch 56/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.8197e-04 - val_loss: 2.7550e-04\n",
      "Epoch 57/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.8096e-04 - val_loss: 2.7617e-04\n",
      "Epoch 58/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.8009e-04 - val_loss: 2.7965e-04\n",
      "Epoch 59/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.7828e-04 - val_loss: 2.7838e-04\n",
      "Epoch 60/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.7745e-04 - val_loss: 2.6548e-04\n",
      "Epoch 61/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.7908e-04 - val_loss: 2.6998e-04\n",
      "Epoch 62/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.7565e-04 - val_loss: 2.7632e-04\n",
      "Epoch 63/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.7586e-04 - val_loss: 2.8059e-04\n",
      "Epoch 64/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.7363e-04 - val_loss: 2.6240e-04\n",
      "Epoch 65/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.7312e-04 - val_loss: 2.6211e-04\n",
      "Epoch 66/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.7165e-04 - val_loss: 2.6847e-04\n",
      "Epoch 67/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.7082e-04 - val_loss: 2.6949e-04\n",
      "Epoch 68/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.6918e-04 - val_loss: 2.6463e-04\n",
      "Epoch 69/300\n",
      "4149/4149 [==============================] - 24s 6ms/step - loss: 2.6789e-04 - val_loss: 2.6085e-04\n",
      "Epoch 70/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.6662e-04 - val_loss: 2.6615e-04\n",
      "Epoch 71/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.6529e-04 - val_loss: 2.5080e-04\n",
      "Epoch 72/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.6419e-04 - val_loss: 2.7919e-04\n",
      "Epoch 73/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.6224e-04 - val_loss: 2.5157e-04\n",
      "Epoch 74/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.6203e-04 - val_loss: 2.4978e-04\n",
      "Epoch 75/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.5944e-04 - val_loss: 2.5662e-04\n",
      "Epoch 76/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.5784e-04 - val_loss: 2.4986e-04\n",
      "Epoch 77/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.5688e-04 - val_loss: 2.4489e-04\n",
      "Epoch 78/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.5484e-04 - val_loss: 2.5702e-04\n",
      "Epoch 79/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.5345e-04 - val_loss: 2.4412e-04\n",
      "Epoch 80/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.5167e-04 - val_loss: 2.6018e-04\n",
      "Epoch 81/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.5085e-04 - val_loss: 2.5750e-04\n",
      "Epoch 82/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.4923e-04 - val_loss: 2.5447e-04\n",
      "Epoch 83/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.4845e-04 - val_loss: 2.3835e-04\n",
      "Epoch 84/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.4959e-04 - val_loss: 2.4349e-04\n",
      "Epoch 85/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.4673e-04 - val_loss: 2.4298e-04\n",
      "Epoch 86/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.4517e-04 - val_loss: 2.3359e-04\n",
      "Epoch 87/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.4358e-04 - val_loss: 2.3258e-04\n",
      "Epoch 88/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.4250e-04 - val_loss: 2.3881e-04\n",
      "Epoch 89/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.4191e-04 - val_loss: 2.3329e-04\n",
      "Epoch 90/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.4015e-04 - val_loss: 2.4606e-04\n",
      "Epoch 91/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.3972e-04 - val_loss: 2.4919e-04\n",
      "Epoch 92/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.3798e-04 - val_loss: 2.4008e-04\n",
      "Epoch 93/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.3806e-04 - val_loss: 2.2587e-04\n",
      "Epoch 94/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.3710e-04 - val_loss: 2.3203e-04\n",
      "Epoch 95/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.3529e-04 - val_loss: 2.2682e-04\n",
      "Epoch 96/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.3452e-04 - val_loss: 2.2582e-04\n",
      "Epoch 97/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.3347e-04 - val_loss: 2.2718e-04\n",
      "Epoch 98/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.3388e-04 - val_loss: 2.3873e-04\n",
      "Epoch 99/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.3142e-04 - val_loss: 2.3775e-04\n",
      "Epoch 100/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.3100e-04 - val_loss: 2.3615e-04\n",
      "Epoch 101/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.3058e-04 - val_loss: 2.1761e-04\n",
      "Epoch 102/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.3009e-04 - val_loss: 2.1944e-04\n",
      "Epoch 103/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.2951e-04 - val_loss: 2.1841e-04\n",
      "Epoch 104/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.2792e-04 - val_loss: 2.1855e-04\n",
      "Epoch 105/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.2653e-04 - val_loss: 2.3223e-04\n",
      "Epoch 106/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.2600e-04 - val_loss: 2.2140e-04\n",
      "Epoch 107/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.2779e-04 - val_loss: 2.3691e-04\n",
      "Epoch 108/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.2604e-04 - val_loss: 2.1790e-04\n",
      "Epoch 109/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.2389e-04 - val_loss: 2.2606e-04\n",
      "Epoch 110/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.2337e-04 - val_loss: 2.1526e-04\n",
      "Epoch 111/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.2268e-04 - val_loss: 2.1088e-04\n",
      "Epoch 112/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.2278e-04 - val_loss: 2.1258e-04\n",
      "Epoch 113/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.2096e-04 - val_loss: 2.0982e-04\n",
      "Epoch 114/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.2065e-04 - val_loss: 2.0970e-04\n",
      "Epoch 115/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.2015e-04 - val_loss: 2.1874e-04\n",
      "Epoch 116/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.1975e-04 - val_loss: 2.1768e-04\n",
      "Epoch 117/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.1881e-04 - val_loss: 2.3126e-04\n",
      "Epoch 118/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.1822e-04 - val_loss: 2.1191e-04\n",
      "Epoch 119/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.1759e-04 - val_loss: 2.0728e-04\n",
      "Epoch 120/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.1687e-04 - val_loss: 2.1009e-04\n",
      "Epoch 121/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.1677e-04 - val_loss: 2.0392e-04\n",
      "Epoch 122/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.1591e-04 - val_loss: 2.0718e-04\n",
      "Epoch 123/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.1643e-04 - val_loss: 2.0675e-04\n",
      "Epoch 124/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.1513e-04 - val_loss: 2.0334e-04\n",
      "Epoch 125/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.1742e-04 - val_loss: 2.0585e-04\n",
      "Epoch 126/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.1417e-04 - val_loss: 2.5088e-04\n",
      "Epoch 127/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.1415e-04 - val_loss: 2.0514e-04\n",
      "Epoch 128/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.1253e-04 - val_loss: 2.0304e-04\n",
      "Epoch 129/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.1313e-04 - val_loss: 2.1710e-04\n",
      "Epoch 130/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.1239e-04 - val_loss: 2.8292e-04\n",
      "Epoch 131/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.1212e-04 - val_loss: 2.0723e-04\n",
      "Epoch 132/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.1170e-04 - val_loss: 2.2476e-04\n",
      "Epoch 133/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.1150e-04 - val_loss: 2.0850e-04\n",
      "Epoch 134/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.1062e-04 - val_loss: 2.0725e-04\n",
      "Epoch 135/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.1009e-04 - val_loss: 1.9947e-04\n",
      "Epoch 136/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.0962e-04 - val_loss: 1.9977e-04\n",
      "Epoch 137/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.0900e-04 - val_loss: 2.0824e-04\n",
      "Epoch 138/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.0943e-04 - val_loss: 2.0086e-04\n",
      "Epoch 139/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.0842e-04 - val_loss: 1.9979e-04\n",
      "Epoch 140/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.0755e-04 - val_loss: 2.6868e-04\n",
      "Epoch 141/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 2.0874e-04 - val_loss: 1.9841e-04\n",
      "Epoch 142/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.0671e-04 - val_loss: 1.9731e-04\n",
      "Epoch 143/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.0699e-04 - val_loss: 2.0901e-04\n",
      "Epoch 144/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.0597e-04 - val_loss: 2.1418e-04\n",
      "Epoch 145/300\n",
      "4149/4149 [==============================] - 21s 5ms/step - loss: 2.0599e-04 - val_loss: 1.9869e-04\n",
      "Epoch 146/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.0701e-04 - val_loss: 2.5492e-04\n",
      "Epoch 147/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.0692e-04 - val_loss: 1.9707e-04\n",
      "Epoch 148/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.0456e-04 - val_loss: 1.9727e-04\n",
      "Epoch 149/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.0469e-04 - val_loss: 1.9593e-04\n",
      "Epoch 150/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.0411e-04 - val_loss: 1.9464e-04\n",
      "Epoch 151/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.0316e-04 - val_loss: 2.0041e-04\n",
      "Epoch 152/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.0507e-04 - val_loss: 1.9900e-04\n",
      "Epoch 153/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.0216e-04 - val_loss: 2.0667e-04\n",
      "Epoch 154/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 2.0331e-04 - val_loss: 1.9223e-04\n",
      "Epoch 155/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.0118e-04 - val_loss: 2.0128e-04\n",
      "Epoch 156/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 2.0000e-04 - val_loss: 1.9253e-04\n",
      "Epoch 157/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.9911e-04 - val_loss: 1.9508e-04\n",
      "Epoch 158/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.9898e-04 - val_loss: 1.9033e-04\n",
      "Epoch 159/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.9902e-04 - val_loss: 1.9107e-04\n",
      "Epoch 160/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.9770e-04 - val_loss: 1.9098e-04\n",
      "Epoch 161/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.9727e-04 - val_loss: 1.8691e-04\n",
      "Epoch 162/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.9655e-04 - val_loss: 1.8650e-04\n",
      "Epoch 163/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.9709e-04 - val_loss: 1.9161e-04\n",
      "Epoch 164/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.9704e-04 - val_loss: 1.9515e-04\n",
      "Epoch 165/300\n",
      "4149/4149 [==============================] - 21s 5ms/step - loss: 1.9576e-04 - val_loss: 1.9220e-04\n",
      "Epoch 166/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.9487e-04 - val_loss: 1.8911e-04\n",
      "Epoch 167/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.9471e-04 - val_loss: 1.8664e-04\n",
      "Epoch 168/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.9396e-04 - val_loss: 1.8850e-04\n",
      "Epoch 169/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.9329e-04 - val_loss: 2.0823e-04\n",
      "Epoch 170/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.9301e-04 - val_loss: 2.0857e-04\n",
      "Epoch 171/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.9264e-04 - val_loss: 1.9570e-04\n",
      "Epoch 172/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 1.9191e-04 - val_loss: 1.8899e-04\n",
      "Epoch 173/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 1.9220e-04 - val_loss: 2.1691e-04\n",
      "Epoch 174/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 1.9217e-04 - val_loss: 1.8289e-04\n",
      "Epoch 175/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.9205e-04 - val_loss: 1.8757e-04\n",
      "Epoch 176/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.9051e-04 - val_loss: 1.8220e-04\n",
      "Epoch 177/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.9036e-04 - val_loss: 1.8524e-04\n",
      "Epoch 178/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.8969e-04 - val_loss: 1.8466e-04\n",
      "Epoch 179/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.8862e-04 - val_loss: 1.7922e-04\n",
      "Epoch 180/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.8960e-04 - val_loss: 1.8076e-04\n",
      "Epoch 181/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.8855e-04 - val_loss: 5.6048e-04\n",
      "Epoch 182/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.8851e-04 - val_loss: 1.7929e-04\n",
      "Epoch 183/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 1.8736e-04 - val_loss: 1.8105e-04\n",
      "Epoch 184/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.8883e-04 - val_loss: 1.7948e-04\n",
      "Epoch 185/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 1.8672e-04 - val_loss: 1.7680e-04\n",
      "Epoch 186/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 1.8594e-04 - val_loss: 2.2182e-04\n",
      "Epoch 187/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.8777e-04 - val_loss: 1.8227e-04\n",
      "Epoch 188/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.8530e-04 - val_loss: 1.7546e-04\n",
      "Epoch 189/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 1.8510e-04 - val_loss: 1.7428e-04\n",
      "Epoch 190/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.8468e-04 - val_loss: 1.8628e-04\n",
      "Epoch 191/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.8523e-04 - val_loss: 1.7924e-04\n",
      "Epoch 192/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.8463e-04 - val_loss: 1.7977e-04\n",
      "Epoch 193/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.8308e-04 - val_loss: 1.8310e-04\n",
      "Epoch 194/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.8302e-04 - val_loss: 1.7461e-04\n",
      "Epoch 195/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.8267e-04 - val_loss: 1.7909e-04\n",
      "Epoch 196/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.8281e-04 - val_loss: 1.7341e-04\n",
      "Epoch 197/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.8348e-04 - val_loss: 1.7860e-04\n",
      "Epoch 198/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.8164e-04 - val_loss: 1.7314e-04\n",
      "Epoch 199/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.8221e-04 - val_loss: 1.8187e-04\n",
      "Epoch 200/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.8172e-04 - val_loss: 1.7593e-04\n",
      "Epoch 201/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.8324e-04 - val_loss: 1.8650e-04\n",
      "Epoch 202/300\n",
      "4149/4149 [==============================] - 21s 5ms/step - loss: 1.8024e-04 - val_loss: 1.7469e-04\n",
      "Epoch 203/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.8130e-04 - val_loss: 1.7682e-04\n",
      "Epoch 204/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.8015e-04 - val_loss: 1.6922e-04\n",
      "Epoch 205/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.8023e-04 - val_loss: 1.7431e-04\n",
      "Epoch 206/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.7859e-04 - val_loss: 1.8664e-04\n",
      "Epoch 207/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.7849e-04 - val_loss: 1.6965e-04\n",
      "Epoch 208/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 1.7822e-04 - val_loss: 1.7010e-04\n",
      "Epoch 209/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 1.7797e-04 - val_loss: 1.7538e-04\n",
      "Epoch 210/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 1.7827e-04 - val_loss: 1.6834e-04\n",
      "Epoch 211/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.7868e-04 - val_loss: 1.7376e-04\n",
      "Epoch 212/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.7698e-04 - val_loss: 1.6788e-04\n",
      "Epoch 213/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 1.7694e-04 - val_loss: 1.6620e-04\n",
      "Epoch 214/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.7625e-04 - val_loss: 1.7686e-04\n",
      "Epoch 215/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.7601e-04 - val_loss: 1.6875e-04\n",
      "Epoch 216/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.7563e-04 - val_loss: 1.6905e-04\n",
      "Epoch 217/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.7714e-04 - val_loss: 1.6844e-04\n",
      "Epoch 218/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.7523e-04 - val_loss: 1.6580e-04\n",
      "Epoch 219/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.7467e-04 - val_loss: 1.6651e-04\n",
      "Epoch 220/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.7422e-04 - val_loss: 1.7125e-04\n",
      "Epoch 221/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.7459e-04 - val_loss: 1.7010e-04\n",
      "Epoch 222/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.7361e-04 - val_loss: 1.7072e-04\n",
      "Epoch 223/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 1.7347e-04 - val_loss: 1.6960e-04\n",
      "Epoch 224/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.7340e-04 - val_loss: 1.6302e-04\n",
      "Epoch 225/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.7266e-04 - val_loss: 1.8299e-04\n",
      "Epoch 226/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.7292e-04 - val_loss: 1.7116e-04\n",
      "Epoch 227/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 1.7236e-04 - val_loss: 1.6442e-04\n",
      "Epoch 228/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.7255e-04 - val_loss: 1.6689e-04\n",
      "Epoch 229/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.7138e-04 - val_loss: 1.6716e-04\n",
      "Epoch 230/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.7167e-04 - val_loss: 1.6392e-04\n",
      "Epoch 231/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.7116e-04 - val_loss: 1.6249e-04\n",
      "Epoch 232/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.7066e-04 - val_loss: 1.6401e-04\n",
      "Epoch 233/300\n",
      "4149/4149 [==============================] - 23s 6ms/step - loss: 1.7004e-04 - val_loss: 1.6704e-04\n",
      "Epoch 234/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 1.6992e-04 - val_loss: 1.7849e-04\n",
      "Epoch 235/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 1.7060e-04 - val_loss: 1.6058e-04\n",
      "Epoch 236/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.7190e-04 - val_loss: 1.7185e-04\n",
      "Epoch 237/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6920e-04 - val_loss: 1.6006e-04\n",
      "Epoch 238/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6870e-04 - val_loss: 1.6652e-04\n",
      "Epoch 239/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6874e-04 - val_loss: 1.6133e-04\n",
      "Epoch 240/300\n",
      "4149/4149 [==============================] - 21s 5ms/step - loss: 1.6815e-04 - val_loss: 1.6214e-04\n",
      "Epoch 241/300\n",
      "4149/4149 [==============================] - 21s 5ms/step - loss: 1.6788e-04 - val_loss: 1.6072e-04\n",
      "Epoch 242/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6766e-04 - val_loss: 1.5954e-04\n",
      "Epoch 243/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6767e-04 - val_loss: 1.6392e-04\n",
      "Epoch 244/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6879e-04 - val_loss: 1.5813e-04\n",
      "Epoch 245/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6685e-04 - val_loss: 1.5996e-04\n",
      "Epoch 246/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6667e-04 - val_loss: 1.5908e-04\n",
      "Epoch 247/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6788e-04 - val_loss: 1.5958e-04\n",
      "Epoch 248/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6657e-04 - val_loss: 1.6157e-04\n",
      "Epoch 249/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6645e-04 - val_loss: 1.7010e-04\n",
      "Epoch 250/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6556e-04 - val_loss: 1.5606e-04\n",
      "Epoch 251/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6532e-04 - val_loss: 1.5589e-04\n",
      "Epoch 252/300\n",
      "4149/4149 [==============================] - 21s 5ms/step - loss: 1.6502e-04 - val_loss: 1.6533e-04\n",
      "Epoch 253/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6496e-04 - val_loss: 1.6586e-04\n",
      "Epoch 254/300\n",
      "4149/4149 [==============================] - 21s 5ms/step - loss: 1.6448e-04 - val_loss: 1.5517e-04\n",
      "Epoch 255/300\n",
      "4149/4149 [==============================] - 21s 5ms/step - loss: 1.6477e-04 - val_loss: 1.5572e-04\n",
      "Epoch 256/300\n",
      "4149/4149 [==============================] - 21s 5ms/step - loss: 1.6435e-04 - val_loss: 1.6735e-04\n",
      "Epoch 257/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6396e-04 - val_loss: 1.5841e-04\n",
      "Epoch 258/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6361e-04 - val_loss: 1.5632e-04\n",
      "Epoch 259/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6322e-04 - val_loss: 1.6424e-04\n",
      "Epoch 260/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6317e-04 - val_loss: 1.5620e-04\n",
      "Epoch 261/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6280e-04 - val_loss: 1.6155e-04\n",
      "Epoch 262/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6297e-04 - val_loss: 1.5869e-04\n",
      "Epoch 263/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6219e-04 - val_loss: 1.6590e-04\n",
      "Epoch 264/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6219e-04 - val_loss: 1.5711e-04\n",
      "Epoch 265/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6222e-04 - val_loss: 1.6583e-04\n",
      "Epoch 266/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6204e-04 - val_loss: 1.5802e-04\n",
      "Epoch 267/300\n",
      "4149/4149 [==============================] - 22s 5ms/step - loss: 1.6169e-04 - val_loss: 1.6087e-04\n",
      "Epoch 268/300\n",
      "4149/4149 [==============================] - 23s 5ms/step - loss: 1.6195e-04 - val_loss: 1.6470e-04\n",
      "Epoch 269/300\n",
      "1028/4149 [======>.......................] - ETA: 16s - loss: 1.5917e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/uio/hume/student-u37/fslippe/master_project/code/new_autoencoder.ipynb Cell 12\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmimi/uio/hume/student-u37/fslippe/master_project/code/new_autoencoder.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmimi/uio/hume/student-u37/fslippe/master_project/code/new_autoencoder.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m early_stopping \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmimi/uio/hume/student-u37/fslippe/master_project/code/new_autoencoder.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(patches, patches, validation_data\u001b[39m=\u001b[39;49m(val_data, val_data), epochs\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[early_stopping])\n",
      "File \u001b[0;32m/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/engine/training.py:1216\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1210\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1211\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1212\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1213\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1214\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1215\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1216\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1217\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1218\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:910\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    907\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    909\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 910\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    912\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    913\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:942\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    939\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    940\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    941\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    945\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3130\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3127\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   3128\u001b[0m   (graph_function,\n\u001b[1;32m   3129\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   3131\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1959\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1955\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1956\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1957\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1958\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1959\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1960\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1961\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m     args,\n\u001b[1;32m   1963\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1964\u001b[0m     executing_eagerly)\n\u001b[1;32m   1965\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/tensorflow/python/eager/function.py:598\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    597\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    599\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    601\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    602\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    604\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    605\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    606\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    607\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    610\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    611\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 58\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     59\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     61\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "gc.collect()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\n",
    "\n",
    "model.fit(patches, patches, validation_data=(val_data, val_data), epochs=300, batch_size=32, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_generator():\n",
    "    \"\"\"Generator to yield data from files.\"\"\"\n",
    "    file_list = ['file1.npy', 'file2.npy', ...]\n",
    "    random.shuffle(file_list)  # Shuffle files at the beginning of each epoch\n",
    "    for file_name in file_list:\n",
    "        data = np.load(file_name)\n",
    "        for item in data:\n",
    "            yield item\n",
    "\n",
    "# Define your dataset\n",
    "dataset = tf.data.Dataset.from_generator(data_generator,\n",
    "                                         output_signature=(tf.TensorSpec(shape=(...), dtype=tf.float32)))  # Fill in the shape and type\n",
    "dataset = dataset.shuffle(buffer_size=10000)  # Shuffle data\n",
    "dataset = dataset.batch(32)  # Batch data\n",
    "dataset = dataset.repeat()  # Repeat dataset indefinitely\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)  # Prefetch data\n",
    "steps_per_epoch = ...  \n",
    "\n",
    "model.fit(dataset, epochs=200, steps_per_epoch=steps_per_epoch, validation_data=(val_data, val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 18:30:42.644659: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(29)_filter_autoencoder/assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(29)_filter_encoder/assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(29)_filter_decoder/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(29)_filter_autoencoder\")\n",
    "autoencoder.encoder.save(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(29)_filter_encoder\")\n",
    "autoencoder.decoder.save(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(29)_filter_decoder\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
