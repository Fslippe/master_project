{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cores: 256\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.layers import Input, Dense, Flatten, Reshape\n",
    "from sklearn.feature_extraction import image as sk_image\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from keras.models import Model\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.signal import convolve2d\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pyhdf.SD import SD, SDC\n",
    "import matplotlib as mpl\n",
    "#tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "from extract_training_data import *\n",
    "from sklearn.feature_extraction.image import extract_patches_2d, reconstruct_from_patches_2d\n",
    "from pyhdf.error import HDF4Error\n",
    "from functions import *\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 11:51:17.687691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21336 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:25:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)\n",
    "\n",
    "#bands = [6, 7, 20, 28, 28, 31]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bands = [6,20,29]\n",
    "#bands = [6,7,20,28,29,31]\n",
    "bands = [29]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cores: 256\n",
      "/scratch/fslippe/modis/MOD02/daytime_1km/MOD021KM.A2020336.0800.061.2020337214455.hdf\n",
      "0 Latitude\n",
      "1 Longitude\n",
      "2 EV_1KM_RefSB\n",
      "3 EV_1KM_RefSB_Uncert_Indexes\n",
      "4 EV_1KM_Emissive\n",
      "5 EV_1KM_Emissive_Uncert_Indexes\n",
      "6 EV_250_Aggr1km_RefSB\n",
      "7 EV_250_Aggr1km_RefSB_Uncert_Indexes\n",
      "8 EV_250_Aggr1km_RefSB_Samples_Used\n",
      "9 EV_500_Aggr1km_RefSB\n",
      "10 EV_500_Aggr1km_RefSB_Uncert_Indexes\n",
      "11 EV_500_Aggr1km_RefSB_Samples_Used\n",
      "12 Height\n",
      "13 SensorZenith\n",
      "14 SensorAzimuth\n",
      "15 Range\n",
      "16 SolarZenith\n",
      "17 SolarAzimuth\n",
      "18 gflags\n",
      "19 EV_Band26\n",
      "20 EV_Band26_Uncert_Indexes\n",
      "21 Band_250M\n",
      "22 Band_500M\n",
      "23 Band_1KM_RefSB\n",
      "24 Band_1KM_Emissive\n",
      "25 Noise in Thermal Detectors\n",
      "26 Change in relative responses of thermal detectors\n",
      "27 DC Restore Change for Thermal Bands\n",
      "28 DC Restore Change for Reflective 250m Bands\n",
      "29 DC Restore Change for Reflective 500m Bands\n",
      "30 DC Restore Change for Reflective 1km Bands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:20<00:00,  5.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "528"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PROCESSING .hdf files\n",
    "import extract_training_data\n",
    "import importlib\n",
    "importlib.reload(extract_training_data)\n",
    "from extract_training_data import *\n",
    "#folder = \"/uio/hume/student-u37/fslippe/data/training_data/MOD02QKM/normalized_data/\"\n",
    "folder = \"/scratch/fslippe/modis/MOD02/daytime_1km/\"\n",
    "\n",
    "#folder = \"/uio/hume/student-u37/fslippe/data/nird_mount/MOD02QKM_202012-202104/normalized_data/\"\n",
    "\n",
    "start = \"20201201\"\n",
    "end = \"20210416\"\n",
    "#end = \"20201230\"\n",
    "\n",
    "#dates = [\"20210303\"]#, \"20210322\"]#, \"20210323\"]\n",
    "#dates_converted = []\n",
    "#for date in dates:\n",
    "#    dates_converted.append(convert_to_day_of_year(date))\n",
    "#bands = [6,20,29]\n",
    "\n",
    "start_converted = convert_to_day_of_year(start)\n",
    "end_converted = convert_to_day_of_year(end)\n",
    "#print(start_converted)\n",
    "#print(end_converted)\n",
    "#x = [xi for xi in  extract_250m_data(folder, bands=[1], date_list=dates_converted) if xi.shape[0] > 256]\n",
    "x = [xi for xi in  extract_1km_data(folder, bands=bands, start_date=start_converted, end_date=end_converted) if xi.shape[0] > 64]\n",
    "\n",
    "len(x)\n",
    "#x = extract_250m_data(folder, bands=[1], start_date=start_converted, end_date=end_converted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input should already be normalized. Call self.normalize to normalize list of data\n"
     ]
    }
   ],
   "source": [
    "import autoencoder\n",
    "import importlib\n",
    "importlib.reload(autoencoder)\n",
    "from autoencoder import SobelFilterLayer, SimpleAutoencoder\n",
    "patch_size = 64\n",
    "\n",
    "autoencoder = SimpleAutoencoder(len(bands), patch_size, patch_size)\n",
    "#x = autoencoder.normalize(x)\n",
    "#optimizer = mixed_precision.LossScaleOptimizer(tf.keras.optimizers.Adam(learning_rate=1e-4), loss_scale='dynamic')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "model = autoencoder.model(optimizer=optimizer, loss=\"combined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(343728, 64, 64, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = np.concatenate([autoencoder.extract_patches(n_d) for n_d in x], axis=0)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished split\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "patches, val_data = train_test_split(patches, test_size=0.1, random_state=42)\n",
    "print(\"finished split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(309355, 64, 64, 1)\n",
      "(34373, 64, 64, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(patches.shape)\n",
    "print(val_data.shape)\n",
    "patches.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[15.703261]]]]\n"
     ]
    }
   ],
   "source": [
    "min_val = 0\n",
    "max_val = np.max(patches, axis=(0,1,2), keepdims=True)\n",
    "print(max_val)\n",
    "patches = (patches - min_val) / (max_val - min_val)\n",
    "val_data = (val_data - min_val) / (max_val - min_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"/scratch/fslippe/modis/MOD02/test_data/normalized_testpatches_band(1)_winter20_21.npy\", val_data)\n",
    "#np.save(\"/scratch/fslippe/modis/MOD02/training_data/normalized_trainingpatches_band(1)_winter20_21.npy\", patches)\n",
    "# val_data = np.load(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_val_patches.npy\")\n",
    "# patches = np.load(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_train_patches.npy\")[:int(170e3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "### SAVE TRAIN TEST PATCHES \n",
    "# patches = np.load(\"/scratch/fslippe/modis/MOD02/training_data/normalized_trainingpatches_bands6,20,29_winter20_21.npy\")[::4]\n",
    "# val_data  = np.load(\"/scratch/fslippe/modis/MOD02/test_data/normalized_testpatches_bands6,20,29_winter20_21.npy\")[::4]\n",
    "# patches.shape\n",
    "\n",
    "# print(np.mean(patches, axis=(0,1,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 11:53:44.022451: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 5068472320 exceeds 10% of free system memory.\n",
      "2023-09-29 11:53:45.085722: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 5068472320 exceeds 10% of free system memory.\n",
      "2023-09-29 11:53:46.167037: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 5068472320 exceeds 10% of free system memory.\n",
      "2023-09-29 11:53:46.990405: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 5068472320 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 11:53:49.563115: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9668/9668 [==============================] - 53s 5ms/step - loss: 0.0016 - val_loss: 9.8047e-04\n",
      "Epoch 2/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 8.3562e-04 - val_loss: 7.7865e-04\n",
      "Epoch 3/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 6.7118e-04 - val_loss: 6.1036e-04\n",
      "Epoch 4/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 5.8823e-04 - val_loss: 5.5269e-04\n",
      "Epoch 5/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 5.3638e-04 - val_loss: 5.1348e-04\n",
      "Epoch 6/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 5.0411e-04 - val_loss: 4.8643e-04\n",
      "Epoch 7/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 4.7871e-04 - val_loss: 4.5556e-04\n",
      "Epoch 8/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 4.5297e-04 - val_loss: 4.4308e-04\n",
      "Epoch 9/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 4.3158e-04 - val_loss: 4.3495e-04\n",
      "Epoch 10/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 4.1296e-04 - val_loss: 3.9585e-04\n",
      "Epoch 11/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 3.9639e-04 - val_loss: 3.7883e-04\n",
      "Epoch 12/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 3.8226e-04 - val_loss: 3.6717e-04\n",
      "Epoch 13/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 3.6981e-04 - val_loss: 3.5977e-04\n",
      "Epoch 14/200\n",
      "9668/9668 [==============================] - 51s 5ms/step - loss: 3.5945e-04 - val_loss: 3.5977e-04\n",
      "Epoch 15/200\n",
      "9668/9668 [==============================] - 51s 5ms/step - loss: 3.5102e-04 - val_loss: 3.5918e-04\n",
      "Epoch 16/200\n",
      "9668/9668 [==============================] - 51s 5ms/step - loss: 3.4293e-04 - val_loss: 3.3295e-04\n",
      "Epoch 17/200\n",
      "9668/9668 [==============================] - 51s 5ms/step - loss: 3.3612e-04 - val_loss: 3.2895e-04\n",
      "Epoch 18/200\n",
      "9668/9668 [==============================] - 51s 5ms/step - loss: 3.3050e-04 - val_loss: 3.3425e-04\n",
      "Epoch 19/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 3.2368e-04 - val_loss: 3.2931e-04\n",
      "Epoch 20/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 3.1736e-04 - val_loss: 3.2529e-04\n",
      "Epoch 21/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 3.1161e-04 - val_loss: 3.2110e-04\n",
      "Epoch 22/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 3.0591e-04 - val_loss: 2.9863e-04\n",
      "Epoch 23/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 3.0054e-04 - val_loss: 3.1114e-04\n",
      "Epoch 24/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.9562e-04 - val_loss: 2.8982e-04\n",
      "Epoch 25/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.9095e-04 - val_loss: 2.8529e-04\n",
      "Epoch 26/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.8703e-04 - val_loss: 2.7526e-04\n",
      "Epoch 27/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.8369e-04 - val_loss: 2.7345e-04\n",
      "Epoch 28/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 2.8031e-04 - val_loss: 2.7488e-04\n",
      "Epoch 29/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.7732e-04 - val_loss: 2.7086e-04\n",
      "Epoch 30/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 2.7497e-04 - val_loss: 2.7025e-04\n",
      "Epoch 31/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 2.7266e-04 - val_loss: 2.7585e-04\n",
      "Epoch 32/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 2.7034e-04 - val_loss: 2.6625e-04\n",
      "Epoch 33/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 2.6832e-04 - val_loss: 2.8260e-04\n",
      "Epoch 34/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 2.6672e-04 - val_loss: 2.6348e-04\n",
      "Epoch 35/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 2.6482e-04 - val_loss: 2.5675e-04\n",
      "Epoch 36/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.6316e-04 - val_loss: 2.5603e-04\n",
      "Epoch 37/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 2.6135e-04 - val_loss: 2.6554e-04\n",
      "Epoch 38/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 2.5942e-04 - val_loss: 2.5199e-04\n",
      "Epoch 39/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.5759e-04 - val_loss: 2.5049e-04\n",
      "Epoch 40/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 2.5540e-04 - val_loss: 2.4499e-04\n",
      "Epoch 41/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.5309e-04 - val_loss: 2.4986e-04\n",
      "Epoch 42/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.5111e-04 - val_loss: 2.4774e-04\n",
      "Epoch 43/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.4929e-04 - val_loss: 2.4071e-04\n",
      "Epoch 44/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.4740e-04 - val_loss: 2.3976e-04\n",
      "Epoch 45/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.4566e-04 - val_loss: 2.4068e-04\n",
      "Epoch 46/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.4383e-04 - val_loss: 2.3940e-04\n",
      "Epoch 47/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.4174e-04 - val_loss: 2.3320e-04\n",
      "Epoch 48/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.3980e-04 - val_loss: 2.3521e-04\n",
      "Epoch 49/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 2.3793e-04 - val_loss: 2.3801e-04\n",
      "Epoch 50/200\n",
      "9668/9668 [==============================] - 51s 5ms/step - loss: 2.3616e-04 - val_loss: 2.3009e-04\n",
      "Epoch 51/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 2.3413e-04 - val_loss: 2.3450e-04\n",
      "Epoch 52/200\n",
      "9668/9668 [==============================] - 51s 5ms/step - loss: 2.3238e-04 - val_loss: 2.2663e-04\n",
      "Epoch 53/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.3099e-04 - val_loss: 2.2312e-04\n",
      "Epoch 54/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.2898e-04 - val_loss: 2.2591e-04\n",
      "Epoch 55/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.2744e-04 - val_loss: 2.1939e-04\n",
      "Epoch 56/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.2580e-04 - val_loss: 2.1855e-04\n",
      "Epoch 57/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.2423e-04 - val_loss: 2.1781e-04\n",
      "Epoch 58/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.2311e-04 - val_loss: 2.3428e-04\n",
      "Epoch 59/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.2188e-04 - val_loss: 2.1544e-04\n",
      "Epoch 60/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 2.2043e-04 - val_loss: 2.2093e-04\n",
      "Epoch 61/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 2.1934e-04 - val_loss: 2.1303e-04\n",
      "Epoch 62/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.1819e-04 - val_loss: 2.1437e-04\n",
      "Epoch 63/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 2.1706e-04 - val_loss: 2.3261e-04\n",
      "Epoch 64/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.1612e-04 - val_loss: 2.1197e-04\n",
      "Epoch 65/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 2.1554e-04 - val_loss: 2.1063e-04\n",
      "Epoch 66/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 2.1418e-04 - val_loss: 2.1051e-04\n",
      "Epoch 67/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.1314e-04 - val_loss: 2.1174e-04\n",
      "Epoch 68/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.1230e-04 - val_loss: 2.0784e-04\n",
      "Epoch 69/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.1131e-04 - val_loss: 2.1353e-04\n",
      "Epoch 70/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.1051e-04 - val_loss: 2.0367e-04\n",
      "Epoch 71/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 2.0987e-04 - val_loss: 2.0022e-04\n",
      "Epoch 72/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 2.0878e-04 - val_loss: 2.0820e-04\n",
      "Epoch 73/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 2.0813e-04 - val_loss: 2.0151e-04\n",
      "Epoch 74/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.0728e-04 - val_loss: 2.6793e-04\n",
      "Epoch 75/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.0662e-04 - val_loss: 2.1361e-04\n",
      "Epoch 76/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 2.0574e-04 - val_loss: 2.0238e-04\n",
      "Epoch 77/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 2.0491e-04 - val_loss: 2.0309e-04\n",
      "Epoch 78/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 2.0427e-04 - val_loss: 2.0639e-04\n",
      "Epoch 79/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 2.0386e-04 - val_loss: 1.9584e-04\n",
      "Epoch 80/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.0293e-04 - val_loss: 1.9764e-04\n",
      "Epoch 81/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 2.0206e-04 - val_loss: 1.9764e-04\n",
      "Epoch 82/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.0197e-04 - val_loss: 1.9576e-04\n",
      "Epoch 83/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 2.0091e-04 - val_loss: 1.9452e-04\n",
      "Epoch 84/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 2.0038e-04 - val_loss: 1.9241e-04\n",
      "Epoch 85/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.9981e-04 - val_loss: 1.9697e-04\n",
      "Epoch 86/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.9904e-04 - val_loss: 1.9352e-04\n",
      "Epoch 87/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.9859e-04 - val_loss: 2.0153e-04\n",
      "Epoch 88/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.9804e-04 - val_loss: 1.9791e-04\n",
      "Epoch 89/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.9741e-04 - val_loss: 1.8813e-04\n",
      "Epoch 90/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.9703e-04 - val_loss: 1.9181e-04\n",
      "Epoch 91/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.9627e-04 - val_loss: 1.9649e-04\n",
      "Epoch 92/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.9592e-04 - val_loss: 1.9921e-04\n",
      "Epoch 93/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.9522e-04 - val_loss: 1.9224e-04\n",
      "Epoch 94/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 2.0268e-04 - val_loss: 1.8664e-04\n",
      "Epoch 95/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.9433e-04 - val_loss: 1.8760e-04\n",
      "Epoch 96/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 1.9389e-04 - val_loss: 1.9953e-04\n",
      "Epoch 97/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.9329e-04 - val_loss: 1.9009e-04\n",
      "Epoch 98/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.9285e-04 - val_loss: 1.8912e-04\n",
      "Epoch 99/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.9233e-04 - val_loss: 1.8709e-04\n",
      "Epoch 100/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 1.9176e-04 - val_loss: 1.8986e-04\n",
      "Epoch 101/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.9140e-04 - val_loss: 1.8808e-04\n",
      "Epoch 102/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.9100e-04 - val_loss: 1.8668e-04\n",
      "Epoch 103/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.9051e-04 - val_loss: 2.0433e-04\n",
      "Epoch 104/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.9011e-04 - val_loss: 1.8503e-04\n",
      "Epoch 105/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.8980e-04 - val_loss: 1.8804e-04\n",
      "Epoch 106/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.8931e-04 - val_loss: 1.8385e-04\n",
      "Epoch 107/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.8879e-04 - val_loss: 1.8012e-04\n",
      "Epoch 108/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.8851e-04 - val_loss: 1.8527e-04\n",
      "Epoch 109/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.8806e-04 - val_loss: 1.8698e-04\n",
      "Epoch 110/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 1.8777e-04 - val_loss: 1.8290e-04\n",
      "Epoch 111/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 1.8754e-04 - val_loss: 1.7994e-04\n",
      "Epoch 112/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 1.8678e-04 - val_loss: 1.8556e-04\n",
      "Epoch 113/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 1.8642e-04 - val_loss: 1.8090e-04\n",
      "Epoch 114/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 1.8597e-04 - val_loss: 1.9037e-04\n",
      "Epoch 115/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 1.8545e-04 - val_loss: 1.8166e-04\n",
      "Epoch 116/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 1.8438e-04 - val_loss: 1.7778e-04\n",
      "Epoch 117/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 1.8334e-04 - val_loss: 1.7559e-04\n",
      "Epoch 118/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 1.8214e-04 - val_loss: 1.7396e-04\n",
      "Epoch 119/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 1.8111e-04 - val_loss: 1.7591e-04\n",
      "Epoch 120/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 1.8029e-04 - val_loss: 1.7403e-04\n",
      "Epoch 121/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 1.7932e-04 - val_loss: 1.8049e-04\n",
      "Epoch 122/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 1.7880e-04 - val_loss: 1.7477e-04\n",
      "Epoch 123/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.7743e-04 - val_loss: 1.7154e-04\n",
      "Epoch 124/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.7671e-04 - val_loss: 1.7653e-04\n",
      "Epoch 125/200\n",
      "9668/9668 [==============================] - 50s 5ms/step - loss: 1.7588e-04 - val_loss: 1.7152e-04\n",
      "Epoch 126/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.7530e-04 - val_loss: 1.7202e-04\n",
      "Epoch 127/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.7454e-04 - val_loss: 1.7009e-04\n",
      "Epoch 128/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.7387e-04 - val_loss: 1.6979e-04\n",
      "Epoch 129/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.7352e-04 - val_loss: 1.6638e-04\n",
      "Epoch 130/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.7261e-04 - val_loss: 1.6937e-04\n",
      "Epoch 131/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.7194e-04 - val_loss: 1.6630e-04\n",
      "Epoch 132/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.7142e-04 - val_loss: 1.6869e-04\n",
      "Epoch 133/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 1.7105e-04 - val_loss: 1.6958e-04\n",
      "Epoch 134/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 1.7033e-04 - val_loss: 1.6714e-04\n",
      "Epoch 135/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.6998e-04 - val_loss: 1.6878e-04\n",
      "Epoch 136/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.6955e-04 - val_loss: 1.6379e-04\n",
      "Epoch 137/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 1.6907e-04 - val_loss: 1.7163e-04\n",
      "Epoch 138/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.6884e-04 - val_loss: 1.6593e-04\n",
      "Epoch 139/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.6823e-04 - val_loss: 1.8037e-04\n",
      "Epoch 140/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.6769e-04 - val_loss: 1.6796e-04\n",
      "Epoch 141/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.6745e-04 - val_loss: 1.6937e-04\n",
      "Epoch 142/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.6706e-04 - val_loss: 1.6063e-04\n",
      "Epoch 143/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.6677e-04 - val_loss: 1.6513e-04\n",
      "Epoch 144/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.6624e-04 - val_loss: 1.6304e-04\n",
      "Epoch 145/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.6581e-04 - val_loss: 1.5871e-04\n",
      "Epoch 146/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.6554e-04 - val_loss: 1.6129e-04\n",
      "Epoch 147/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.6518e-04 - val_loss: 1.5862e-04\n",
      "Epoch 148/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.6484e-04 - val_loss: 1.6089e-04\n",
      "Epoch 149/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 1.6451e-04 - val_loss: 1.5967e-04\n",
      "Epoch 150/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 1.6417e-04 - val_loss: 1.5774e-04\n",
      "Epoch 151/200\n",
      "9668/9668 [==============================] - 49s 5ms/step - loss: 1.6382e-04 - val_loss: 1.5861e-04\n",
      "Epoch 152/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.6353e-04 - val_loss: 1.5556e-04\n",
      "Epoch 153/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.6315e-04 - val_loss: 1.5704e-04\n",
      "Epoch 154/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.6287e-04 - val_loss: 1.5743e-04\n",
      "Epoch 155/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.6298e-04 - val_loss: 1.5712e-04\n",
      "Epoch 156/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.6234e-04 - val_loss: 1.5619e-04\n",
      "Epoch 157/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.6207e-04 - val_loss: 1.5646e-04\n",
      "Epoch 158/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.6177e-04 - val_loss: 1.6299e-04\n",
      "Epoch 159/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.6143e-04 - val_loss: 1.7284e-04\n",
      "Epoch 160/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.6117e-04 - val_loss: 1.5841e-04\n",
      "Epoch 161/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.6069e-04 - val_loss: 1.6122e-04\n",
      "Epoch 162/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.6053e-04 - val_loss: 1.5405e-04\n",
      "Epoch 163/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.6032e-04 - val_loss: 1.6384e-04\n",
      "Epoch 164/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5988e-04 - val_loss: 1.5482e-04\n",
      "Epoch 165/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5959e-04 - val_loss: 1.5223e-04\n",
      "Epoch 166/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5934e-04 - val_loss: 1.5294e-04\n",
      "Epoch 167/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5914e-04 - val_loss: 1.5145e-04\n",
      "Epoch 168/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5870e-04 - val_loss: 1.5592e-04\n",
      "Epoch 169/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5841e-04 - val_loss: 1.5263e-04\n",
      "Epoch 170/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5808e-04 - val_loss: 1.6052e-04\n",
      "Epoch 171/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5823e-04 - val_loss: 1.5460e-04\n",
      "Epoch 172/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5759e-04 - val_loss: 1.6656e-04\n",
      "Epoch 173/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5712e-04 - val_loss: 1.4917e-04\n",
      "Epoch 174/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5690e-04 - val_loss: 1.5031e-04\n",
      "Epoch 175/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5648e-04 - val_loss: 1.5156e-04\n",
      "Epoch 176/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5620e-04 - val_loss: 1.4898e-04\n",
      "Epoch 177/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5592e-04 - val_loss: 1.5739e-04\n",
      "Epoch 178/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.5553e-04 - val_loss: 1.5204e-04\n",
      "Epoch 179/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5544e-04 - val_loss: 1.5858e-04\n",
      "Epoch 180/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.5504e-04 - val_loss: 1.5059e-04\n",
      "Epoch 181/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.5468e-04 - val_loss: 1.4901e-04\n",
      "Epoch 182/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.5450e-04 - val_loss: 1.5400e-04\n",
      "Epoch 183/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.5420e-04 - val_loss: 1.4879e-04\n",
      "Epoch 184/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.5406e-04 - val_loss: 1.4789e-04\n",
      "Epoch 185/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.5373e-04 - val_loss: 1.5523e-04\n",
      "Epoch 186/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5354e-04 - val_loss: 1.4850e-04\n",
      "Epoch 187/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5329e-04 - val_loss: 1.6382e-04\n",
      "Epoch 188/200\n",
      "9668/9668 [==============================] - 48s 5ms/step - loss: 1.5298e-04 - val_loss: 1.5464e-04\n",
      "Epoch 189/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5289e-04 - val_loss: 1.5128e-04\n",
      "Epoch 190/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5257e-04 - val_loss: 1.4831e-04\n",
      "Epoch 191/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5240e-04 - val_loss: 1.4666e-04\n",
      "Epoch 192/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5205e-04 - val_loss: 1.4781e-04\n",
      "Epoch 193/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5184e-04 - val_loss: 1.4550e-04\n",
      "Epoch 194/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5391e-04 - val_loss: 1.4569e-04\n",
      "Epoch 195/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5145e-04 - val_loss: 1.5221e-04\n",
      "Epoch 196/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5127e-04 - val_loss: 1.4562e-04\n",
      "Epoch 197/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5104e-04 - val_loss: 1.4673e-04\n",
      "Epoch 198/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5099e-04 - val_loss: 1.4988e-04\n",
      "Epoch 199/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5072e-04 - val_loss: 2.3766e-04\n",
      "Epoch 200/200\n",
      "9668/9668 [==============================] - 47s 5ms/step - loss: 1.5054e-04 - val_loss: 1.4369e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4ffc16d3a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "gc.collect()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "model.fit(patches, patches, validation_data=(val_data, val_data), epochs=200, batch_size=32, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_generator():\n",
    "    \"\"\"Generator to yield data from files.\"\"\"\n",
    "    file_list = ['file1.npy', 'file2.npy', ...]\n",
    "    random.shuffle(file_list)  # Shuffle files at the beginning of each epoch\n",
    "    for file_name in file_list:\n",
    "        data = np.load(file_name)\n",
    "        for item in data:\n",
    "            yield item\n",
    "\n",
    "# Define your dataset\n",
    "dataset = tf.data.Dataset.from_generator(data_generator,\n",
    "                                         output_signature=(tf.TensorSpec(shape=(...), dtype=tf.float32)))  # Fill in the shape and type\n",
    "dataset = dataset.shuffle(buffer_size=10000)  # Shuffle data\n",
    "dataset = dataset.batch(32)  # Batch data\n",
    "dataset = dataset.repeat()  # Repeat dataset indefinitely\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)  # Prefetch data\n",
    "steps_per_epoch = ...  \n",
    "\n",
    "model.fit(dataset, epochs=200, steps_per_epoch=steps_per_epoch, validation_data=(val_data, val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 14:34:28.339925: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(29)__autoencoder/assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(29)_encoder/assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(29)_decoder/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(29)__autoencoder\")\n",
    "autoencoder.encoder.save(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(29)_encoder\")\n",
    "autoencoder.decoder.save(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(29)_decoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
