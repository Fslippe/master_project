{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Flatten, Reshape\n",
    "from sklearn.feature_extraction import image as sk_image\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from keras.models import Model\n",
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.signal import convolve2d\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pyhdf.SD import SD, SDC\n",
    "import matplotlib as mpl\n",
    "#tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "from extract_training_data import *\n",
    "from sklearn.feature_extraction.image import extract_patches_2d, reconstruct_from_patches_2d\n",
    "from pyhdf.error import HDF4Error\n",
    "from functions import *\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 18:53:11.820350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21020 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:25:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)\n",
    "\n",
    "#bands = [6, 7, 20, 28, 28, 31]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/fslippe/modis/MOD02/daytime_1km/MOD021KM.A2020336.0800.061.2020337214455.hdf\n",
      "0 Latitude\n",
      "1 Longitude\n",
      "2 EV_1KM_RefSB\n",
      "3 EV_1KM_RefSB_Uncert_Indexes\n",
      "4 EV_1KM_Emissive\n",
      "5 EV_1KM_Emissive_Uncert_Indexes\n",
      "6 EV_250_Aggr1km_RefSB\n",
      "7 EV_250_Aggr1km_RefSB_Uncert_Indexes\n",
      "8 EV_250_Aggr1km_RefSB_Samples_Used\n",
      "9 EV_500_Aggr1km_RefSB\n",
      "10 EV_500_Aggr1km_RefSB_Uncert_Indexes\n",
      "11 EV_500_Aggr1km_RefSB_Samples_Used\n",
      "12 Height\n",
      "13 SensorZenith\n",
      "14 SensorAzimuth\n",
      "15 Range\n",
      "16 SolarZenith\n",
      "17 SolarAzimuth\n",
      "18 gflags\n",
      "19 EV_Band26\n",
      "20 EV_Band26_Uncert_Indexes\n",
      "21 Band_250M\n",
      "22 Band_500M\n",
      "23 Band_1KM_RefSB\n",
      "24 Band_1KM_Emissive\n",
      "25 Noise in Thermal Detectors\n",
      "26 Change in relative responses of thermal detectors\n",
      "27 DC Restore Change for Thermal Bands\n",
      "28 DC Restore Change for Reflective 250m Bands\n",
      "29 DC Restore Change for Reflective 500m Bands\n",
      "30 DC Restore Change for Reflective 1km Bands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/easybuild/software/SciPy-bundle/2021.10-foss-2021b/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/software/easybuild/software/SciPy-bundle/2021.10-foss-2021b/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 124/124 [00:10<00:00, 11.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "732"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PROCESSING .hdf files\n",
    "import extract_training_data\n",
    "import importlib\n",
    "importlib.reload(extract_training_data)\n",
    "from extract_training_data import *\n",
    "#folder = \"/uio/hume/student-u37/fslippe/data/training_data/MOD02QKM/normalized_data/\"\n",
    "folder = \"/scratch/fslippe/modis/MOD02/daytime_1km/\"\n",
    "\n",
    "#folder = \"/uio/hume/student-u37/fslippe/data/nird_mount/MOD02QKM_202012-202104/normalized_data/\"\n",
    "\n",
    "start = \"20201201\"\n",
    "end = \"20210430\"\n",
    "#dates = [\"20210303\"]#, \"20210322\"]#, \"20210323\"]\n",
    "#dates_converted = []\n",
    "#for date in dates:\n",
    "#    dates_converted.append(convert_to_day_of_year(date))\n",
    "\n",
    "start_converted = convert_to_day_of_year(start)\n",
    "end_converted = convert_to_day_of_year(end)\n",
    "#print(start_converted)\n",
    "#print(end_converted)\n",
    "#x = [xi for xi in  extract_250m_data(folder, bands=[1], date_list=dates_converted) if xi.shape[0] > 256]\n",
    "x = [xi for xi in  extract_1km_data(folder, bands=[1], start_date=start_converted, end_date=end_converted) if xi.shape[0] > 64]\n",
    "hdf = SD(\"/scratch/fslippe/modis/MOD02/daytime_1km/MOD021KM.A2021096.1620.061.2021097013938.hdf\", SDC.READ)\n",
    "\n",
    "len(x)\n",
    "#x = extract_250m_data(folder, bands=[1], start_date=start_converted, end_date=end_converted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input should already be normalized. Call self.normalize to normalize list of data\n"
     ]
    }
   ],
   "source": [
    "import autoencoder\n",
    "import importlib\n",
    "importlib.reload(autoencoder)\n",
    "from autoencoder import SobelFilterLayer, SimpleAutoencoder\n",
    "bands = [1]\n",
    "patch_size = 64\n",
    "\n",
    "autoencoder = SimpleAutoencoder(len(bands), patch_size, patch_size)\n",
    "#x = autoencoder.normalize(x)\n",
    "#optimizer = mixed_precision.LossScaleOptimizer(tf.keras.optimizers.Adam(learning_rate=1e-4), loss_scale='dynamic')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "model = autoencoder.model(optimizer=optimizer, threshold=0.09,loss=\"combined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = np.concatenate([autoencoder.extract_patches(n_d) for n_d in x], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished split\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "patches, val_data = train_test_split(patches, test_size=0.20, random_state=42)\n",
    "print(\"finished split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(378268, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539.9074968807399\n"
     ]
    }
   ],
   "source": [
    "min_val = 0\n",
    "max_val = np.max(patches)\n",
    "print(max_val)\n",
    "patches = (patches - min_val) / (max_val - min_val)\n",
    "val_data = (val_data - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 16:36:06.316191: W tensorflow/core/common_runtime/bfc_allocator.cc:343] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 16:36:11.217255: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11821/11821 [==============================] - 71s 6ms/step - loss: 0.0067 - val_loss: 0.0046\n",
      "Epoch 2/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0040 - val_loss: 0.0035\n",
      "Epoch 3/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0033 - val_loss: 0.0029\n",
      "Epoch 4/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0029 - val_loss: 0.0026\n",
      "Epoch 5/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0026 - val_loss: 0.0024\n",
      "Epoch 6/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 7/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 8/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 9/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 10/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 11/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 12/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 13/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 14/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 15/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 16/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 17/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 18/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 19/100\n",
      "11821/11821 [==============================] - 62s 5ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 20/100\n",
      "11821/11821 [==============================] - 62s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 21/100\n",
      "11821/11821 [==============================] - 62s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 22/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 23/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 24/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 25/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 26/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 27/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 28/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 29/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 30/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 31/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 32/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 33/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 34/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 35/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 36/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 37/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 38/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 39/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 40/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 41/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 42/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 43/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 44/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 45/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 46/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 47/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 48/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 49/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 50/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0010 - val_loss: 9.6663e-04\n",
      "Epoch 51/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 52/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0010 - val_loss: 9.6313e-04\n",
      "Epoch 53/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0010 - val_loss: 9.7523e-04\n",
      "Epoch 54/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 9.9114e-04 - val_loss: 9.6687e-04\n",
      "Epoch 55/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 9.8737e-04 - val_loss: 9.6097e-04\n",
      "Epoch 56/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 0.0010 - val_loss: 9.3151e-04\n",
      "Epoch 57/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 9.7893e-04 - val_loss: 9.2781e-04\n",
      "Epoch 58/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 9.9679e-04 - val_loss: 9.7663e-04\n",
      "Epoch 59/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 9.6629e-04 - val_loss: 9.2474e-04\n",
      "Epoch 60/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 9.5671e-04 - val_loss: 9.5452e-04\n",
      "Epoch 61/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 9.5069e-04 - val_loss: 0.0010\n",
      "Epoch 62/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 9.4745e-04 - val_loss: 9.1312e-04\n",
      "Epoch 63/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 9.3828e-04 - val_loss: 9.3514e-04\n",
      "Epoch 64/100\n",
      "11821/11821 [==============================] - 62s 5ms/step - loss: 9.4307e-04 - val_loss: 8.9452e-04\n",
      "Epoch 65/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 9.3585e-04 - val_loss: 8.9805e-04\n",
      "Epoch 66/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 9.2248e-04 - val_loss: 9.0470e-04\n",
      "Epoch 67/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 9.2036e-04 - val_loss: 8.8536e-04\n",
      "Epoch 68/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 9.1230e-04 - val_loss: 8.9048e-04\n",
      "Epoch 69/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 9.0506e-04 - val_loss: 8.7594e-04\n",
      "Epoch 70/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.9846e-04 - val_loss: 8.7278e-04\n",
      "Epoch 71/100\n",
      "11821/11821 [==============================] - 62s 5ms/step - loss: 8.9345e-04 - val_loss: 8.9261e-04\n",
      "Epoch 72/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.8690e-04 - val_loss: 8.7703e-04\n",
      "Epoch 73/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.8224e-04 - val_loss: 8.4684e-04\n",
      "Epoch 74/100\n",
      "11821/11821 [==============================] - 64s 5ms/step - loss: 8.9461e-04 - val_loss: 8.7851e-04\n",
      "Epoch 75/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.8010e-04 - val_loss: 8.4104e-04\n",
      "Epoch 76/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.7077e-04 - val_loss: 8.2965e-04\n",
      "Epoch 77/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.6259e-04 - val_loss: 8.3034e-04\n",
      "Epoch 78/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.5861e-04 - val_loss: 8.1357e-04\n",
      "Epoch 79/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.5268e-04 - val_loss: 8.1425e-04\n",
      "Epoch 80/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.4995e-04 - val_loss: 8.0157e-04\n",
      "Epoch 81/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.4183e-04 - val_loss: 8.2703e-04\n",
      "Epoch 82/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.5456e-04 - val_loss: 7.9881e-04\n",
      "Epoch 83/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.3420e-04 - val_loss: 8.2944e-04\n",
      "Epoch 84/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.3037e-04 - val_loss: 7.9295e-04\n",
      "Epoch 85/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.3268e-04 - val_loss: 7.8899e-04\n",
      "Epoch 86/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.2154e-04 - val_loss: 8.0927e-04\n",
      "Epoch 87/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.1998e-04 - val_loss: 8.0613e-04\n",
      "Epoch 88/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.3604e-04 - val_loss: 7.6903e-04\n",
      "Epoch 89/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.1605e-04 - val_loss: 8.6681e-04\n",
      "Epoch 90/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.1700e-04 - val_loss: 7.6873e-04\n",
      "Epoch 91/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.0487e-04 - val_loss: 7.6995e-04\n",
      "Epoch 92/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 8.0785e-04 - val_loss: 7.5997e-04\n",
      "Epoch 93/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 7.9989e-04 - val_loss: 7.6288e-04\n",
      "Epoch 94/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 7.9619e-04 - val_loss: 7.6809e-04\n",
      "Epoch 95/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 7.9325e-04 - val_loss: 7.5197e-04\n",
      "Epoch 96/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 7.9342e-04 - val_loss: 7.6992e-04\n",
      "Epoch 97/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 7.8987e-04 - val_loss: 8.5997e-04\n",
      "Epoch 98/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 7.8424e-04 - val_loss: 7.4960e-04\n",
      "Epoch 99/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 7.8137e-04 - val_loss: 7.3655e-04\n",
      "Epoch 100/100\n",
      "11821/11821 [==============================] - 63s 5ms/step - loss: 7.8373e-04 - val_loss: 7.5800e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe6f0546bb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(patches, patches, validation_data=(val_data, val_data), epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: /uio/hume/student-u37/fslippe/data/models/winter_2020_21_encoder/assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/software/easybuild/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /uio/hume/student-u37/fslippe/data/models/winter_2020_21_decoder/assets\n"
     ]
    }
   ],
   "source": [
    "#model.save(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_autoencoder\")\n",
    "autoencoder.encoder.save(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_encoder\")\n",
    "autoencoder.decoder.save(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_decoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_val_patches\", val_data)\n",
    "np.save(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_train_patches\", patches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import SobelFilterLayer, SimpleAutoencoder\n",
    "bands = [1]\n",
    "patch_size = 256\n",
    "from tensorflow.keras.models import load_model\n",
    "print(len(bands))\n",
    "autoencoder_predict = SimpleAutoencoder(len(bands), patch_size, patch_size)\n",
    "\n",
    "encoder = load_model(\"/uio/hume/student-u37/fslippe/data/models/test_1day_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_patches = np.concatenate([autoencoder.extract_patches(n_d) for n_d in normalized_data[:4]], axis=0)\n",
    "\n",
    "start = 3\n",
    "index_list = [3, 10, 11, 12, 13]\n",
    "cluster_map = autoencoder_predict.kmeans([normalized_data[i] for i in index_list], n_clusters=10, encoder=encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [3, 7, 8, 9]\n",
    "print(np.max(normalized_data[8][:,:,0]))\n",
    "print(np.max(normalized_data[3][:,:,0]))\n",
    "print(np.where(normalized_data[3][:,:,0] <0.8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# Determine global min and max labels\n",
    "global_min = np.min([np.min(cm) for cm in cluster_map])\n",
    "global_max = np.max([np.max(cm) for cm in cluster_map])\n",
    "\n",
    "norm = Normalize(vmin=global_min, vmax=global_max)\n",
    "\n",
    "for i in range(len(cluster_map)):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=[10, 8])\n",
    "    \n",
    "    cb = axs[0].imshow(cluster_map[i], cmap=\"tab10\", norm=norm)\n",
    "    plt.colorbar(cb, ax=axs[0])\n",
    "    \n",
    "    cb = axs[1].imshow(normalized_data[index_list[i]][::4, ::4, 0])\n",
    "    plt.colorbar(cb, ax=axs[1])\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"/uio/hume/student-u37/fslippe/data/cao/\"\n",
    "i=0\n",
    "print(folder + all_files[i])\n",
    "hdf = SD(folder + all_files[i], SDC.READ)\n",
    "print(hdf.attributes())\n",
    "lon = hdf.select('Longitude')[:]\n",
    "lat = hdf.select('Latitude')[:]\n",
    "downsample_factor_x = int(lon.shape[0] / cluster_map[i].shape[0])\n",
    "downsample_factor_y = int(lon.shape[1] / cluster_map[i].shape[1])\n",
    "cluster_shape = cluster_map[i].shape\n",
    "lon_d = lon[::downsample_factor_x, ::downsample_factor_y][:cluster_shape[0], :cluster_shape[1]]\n",
    "lat_d = lat[::downsample_factor_x, ::downsample_factor_y][:cluster_shape[0], :cluster_shape[1]]\n",
    "data = hdf.select(\"EV_250_Aggr1km_RefSB\")[:][0]\n",
    "print(data.shape)\n",
    "hdf.select(\"EV_250_Aggr1km_RefSB\").attributes()\n",
    "print(lon.shape)\n",
    "cluster_map[i].shape\n",
    "datasets = hdf.datasets()   \n",
    "for idx, sds in enumerate(datasets.keys()):\n",
    "    print(idx, sds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axs= plt.subplots(1,2)\n",
    "axs[0].imshow(cluster_map[i])\n",
    "axs[1].imshow(X[i][:,:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = ccrs.NorthPolarStereo()\n",
    "fig, axs = plt.subplots(1, 2, figsize=[10,5], subplot_kw={'projection': projection}, dpi=300)\n",
    "cb = axs[0].pcolormesh(lon_d, lat_d, cluster_map[i], transform=ccrs.PlateCarree(), cmap=\"jet\")  \n",
    "axs[0].coastlines()\n",
    "gl = axs[0].gridlines(draw_labels=True)\n",
    "plt.colorbar(cb)\n",
    "\n",
    "cb = axs[1].pcolormesh(lon, lat, data, transform=ccrs.PlateCarree(), vmin=0)  \n",
    "axs[1].coastlines()\n",
    "gl = axs[1].gridlines(draw_labels=True)\n",
    "plt.colorbar(cb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patches_predict = autoencoder.extract_patches(dataset_5.reshape((1,dataset_5.shape[0], dataset_5.shape[1], 1)))\n",
    "# predict = autoencoder.autoencoder.predict(patches_predict)\n",
    "# predict.shape\n",
    "\n",
    "# #for i in range(83, 90, 2):\n",
    "# pictures = 12\n",
    "# patch_start = 68\n",
    "# fig, axs = plt.subplots(2, pictures, figsize=[pictures*2,5])\n",
    "# i=0\n",
    "# for patch_number in range(patch_start, patch_start+pictures):\n",
    "#     axs[0, i].imshow(patches_predict[patch_number], cmap=\"gray\")\n",
    "#     axs[1, i].imshow(predict[patch_number,:,:,0], cmap=\"gray\")\n",
    "    \n",
    "#     # Turn off ticks and axis labels for both x and y\n",
    "#     axs[0, i].set_yticks([])\n",
    "#     axs[0, i].set_xticks([])\n",
    "#     axs[0, i].axis('off')\n",
    "    \n",
    "#     axs[1, i].set_yticks([])\n",
    "#     axs[1, i].set_xticks([])\n",
    "#     axs[1, i].axis('off')\n",
    "    \n",
    "#     i += 1\n",
    "# plt.tight_layout()\n",
    "\n",
    "patches_predict = autoencoder2.extract_patches(dataset_5.reshape((1,dataset_5.shape[0], dataset_5.shape[1], 1)))\n",
    "predict = autoencoder2.autoencoder.predict(patches_predict)\n",
    "print(predict.shape)\n",
    "\n",
    "#for i in range(83, 90, 2):\n",
    "pictures = 10\n",
    "patch_start = 320\n",
    "fig, axs = plt.subplots(2, pictures, figsize=[pictures*2,5])\n",
    "i=0\n",
    "for patch_number in range(patch_start, patch_start+pictures):\n",
    "    axs[0, i].imshow(patches_predict[patch_number], cmap=\"gray\")\n",
    "    axs[1, i].imshow(predict[patch_number,:,:,0], cmap=\"gray\")\n",
    "    \n",
    "    # Turn off ticks and axis labels for both x and y\n",
    "    axs[0, i].set_yticks([])\n",
    "    axs[0, i].set_xticks([])\n",
    "    axs[0, i].axis('off')\n",
    "    \n",
    "    axs[1, i].set_yticks([])\n",
    "    axs[1, i].set_xticks([])\n",
    "    axs[1, i].axis('off')\n",
    "    \n",
    "    i += 1\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(dataset_5, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "cmp = axes[0].imshow(dataset_2, cmap='gray')\n",
    "plt.colorbar(cmp, ax=axes[0])\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "cmp = axes[1].imshow(labels_img, cmap='tab10')\n",
    "plt.colorbar(cmp, ax=axes[1])\n",
    "axes[1].set_title('Clustered Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(flattened_input, encoded)\n",
    "encoded_imgs = encoder.predict(data)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reduce the encoded images to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "encoded_imgs_2d = pca.fit_transform(encoded_imgs)\n",
    "\n",
    "# Now cluster the 2D encoded images\n",
    "n_clusters = 5  # Define the number of clusters you want\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "cluster_assignments = kmeans.fit_predict(encoded_imgs_2d)\n",
    "\n",
    "print(cluster_assignments.shape)\n",
    "\n",
    "# If you want to visualize the clusters using matplotlib:\n",
    "plt.scatter(encoded_imgs_2d[:, 0], encoded_imgs_2d[:, 1], c=cluster_assignments, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.title('Clusters in 2D')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(dataset_2)\n",
    "fig, axs = plt.subplots(1, 2, figsize=[10,5])\n",
    "\n",
    "cmp = axs[0].imshow(dataset_2, cmap=\"gray\")\n",
    "plt.colorbar(cmp)\n",
    "cmp = axs[1].imshow(decoded_imgs,cmap=\"gray\")\n",
    "plt.colorbar(cmp)\n",
    "\n",
    "#decoded_imgs_2d = decoded_imgs.reshape((-1, *input_shape))\n",
    "# plt.contourf(decoded_imgs_2d)\n",
    "# plt.show(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs_reshaped = decoded_imgs.reshape(-1, 1)\n",
    "\n",
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(decoded_imgs_reshaped)\n",
    "labels = kmeans.labels_\n",
    "labels_reshaped = labels.reshape(2040, 1354)\n",
    "\n",
    "# Assuming `original_img` is your original image\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "cmp = axes[0].imshow(dataset_2, cmap='gray')  # or just cmap depending on the nature of your image\n",
    "plt.colorbar(cmp)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "cmp = axes[1].imshow(labels_reshaped, cmap='tab10')  # 'tab10' is a colormap with distinct colors\n",
    "plt.colorbar(cmp)\n",
    "axes[1].set_title('Clustered Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "block_size = (10, 10)  # Change accordingly\n",
    "n_clusters = 5\n",
    "\n",
    "# Divide the image into blocks and calculate the mean for each block\n",
    "blocks = []\n",
    "for i in range(0, decoded_imgs.shape[0], block_size[0]):\n",
    "    for j in range(0, decoded_imgs.shape[1], block_size[1]):\n",
    "        block = decoded_imgs[i:i+block_size[0], j:j+block_size[1]]\n",
    "        block_mean = np.mean(block)\n",
    "        blocks.append(block_mean)\n",
    "\n",
    "blocks = np.array(blocks).reshape(-1, 1)\n",
    "\n",
    "# Cluster the blocks\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(blocks)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Construct the clustered image\n",
    "clustered_img = np.zeros_like(decoded_imgs)\n",
    "label_idx = 0\n",
    "for i in range(0, decoded_imgs.shape[0], block_size[0]):\n",
    "    for j in range(0, decoded_imgs.shape[1], block_size[1]):\n",
    "        clustered_img[i:i+block_size[0], j:j+block_size[1]] = labels[label_idx]\n",
    "        label_idx += 1\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Assuming `original_img` is your original image\n",
    "axes[0].imshow(dataset_2, cmap='gray')\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(clustered_img, cmap='tab10')  # 'tab10' for distinct colors\n",
    "axes[1].set_title('Clustered Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EasyBuild Python 3.9.6",
   "language": "python",
   "name": "easybuild-python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
